\documentclass[10pt]{article}

\usepackage{natbib}
\usepackage[margin=2cm]{geometry}

%Draft du 27 mars
%Chapitre 5 

% Méthodes d’exploration des modèles de simulation
\title{Exploration methods for simulation models}

\author{Juste Raimbault$^{1,2,3}$ et Denise Pumain$^{3}$\\
\medskip\\
$^{1}$ UPS CNRS 3611 ISC-PIF\\
$^{2}$ CASA, UCL\\
$^{3}$ UMR CNRS 8504 G{\'e}ographie-cit{\'e}s
}
\date{}

\begin{document}
	

%%%%%%
%% IDEES / RQ

% - Q : a ton vraiment passe un plafond de verre qualitatif ou ne fait on pas que repousser les difficultés ? a voir dans les annees qui viennent ! : ajouter en conclusion ? => no ; think to it (what differentiates quali from quanti ?)

% - RQ ! confusion complexite / complication (sur le human brain) => OK lang courant - et vraie question ouverte en fait : quest ce que le cerveau/esprit compute ? tres bon a certaines choses, pas a d'autres ; etc.

% % TDO - signaler morpho analysis a Tannier ?

% - RQ - lowry statique ? - roughly - but can be used dynamically

% rq : principe d'exlusion ? // attractoin - repulsion ~ reaction diffusion ?


% % IDEE - disc Paul modele a lintermediaire, simpoplocal deja un ? modele barthelemy shitty. - why ? [rq th gravite]

% % TDO simpop2 dans le urbangrowth benchmark ?

% Q : (sur la structure de simpoplocal) crucial is to have non linear term / as for positive/negative externalities tradeoff : not really sensitive to the form of the function, or it is ?

% % rq : new experimenbts : // Marion physics -> ?

%  rq : term bifurcation : clarify ? // transitions. write something on a clear mathematical def for transitions, compatible with transmondyn ?

%  rq : even no order of magnitude for simpoplocal params ? strange - maybe lack a normalization ?

%  refl : how manage models urb evol th in forthcoming project / context ?

% q optim : not really a constrined optim, quite different ? - ok just terminology

%  idee : sensitivity of simpoplocal caluibration to these synthetic data / initial configuration / calibration targets ?


%  rq (calib intgib) : et si obj pas log mse mais mse tout court, possible que comportement different ? pas evident

% coding camp : sci positioning ?

%  rq (ccl) sur le dernier point, en opposition avec Paul Bourgine ? ("belle theories etc.") write a piece on "nice theories" in social science ? (blog cybergeo ?)

\maketitle

\begin{abstract}
%On rappelle d’abord dans ce chapitre à quel point les modèles de simulation sont une absolue nécessité en sciences humaines et sociales, qui ne peuvent que très exceptionnellement recourir aux méthodes des sciences expérimentales pour construire leurs savoirs. Les modèles offrent une possibilité de simuler les processus sociaux en remplaçant le jeu complexe des actions et réactions individuelles et collectives aux situations qu’elles engendrent par des mécanismes mathématiques ou informatiques plus simples, permettant de faciliter la compréhension des relations entre causes et conséquences de ces interactions et d’effectuer des prévisions. La formalisation par des modèles mathématiques susceptibles d’offrir des solutions analytiques n’étant souvent guère envisageable pour donner des représentations satisfaisantes de la complexité sociale (Jensen 2018), ce sont de plus en plus des modèles informatiques à base d’agents qui sont utilisés. Pendant longtemps les capacités limitées de calcul des ordinateurs ont empêché de programmer des modèles qui prennent en compte les interactions entre de grands nombres d’entités localisées géographiquement (personnes ou territoires). En principe, ces modèles doivent nous renseigner sur les possibilités et les conditions de l’émergence de certaines situations définies à un échelon macro-géographique à partir des interactions intervenant à un niveau micro-géographique, dans des systèmes aux comportements trop complexes pour être compris par un cerveau humain. Encore faut-il pouvoir étudier le comportement dynamique de ces modèles comportant des effets de feedback non linéaires et vérifier qu’ils produisent des résultats plausibles dans toutes les étapes de leur simulation. Ce travail indispensable de l’exploration de la dynamique des algorithmes est resté balbutiant jusque vers la fin des années 2010, où des algorithmes combinant des méthodes plus sophistiquées incluant des algorithmes génétiques et le recours au calcul intensif distribué ont permis un saut qualitatif important dans la validation des modèles, voire un tournant épistémologique pour les sciences humaines et sociales, comme l’indiquent les dernières applications réalisées avec l’aide de la plateforme OpenMOLE présentée ici. 
%  reformuler phrase interaction / emergence - OK
We recall first in this chapter to what extent simulation models are an absolute necessity in social sciences and humanities, which can only very exceptionally require to experimental sciences methods to construct their knowledge. Models open the perspective to simulate social processes by replacing the complex interplay of individual and collective actions and reactions to the situations they make emerge by simpler mathematical or computational mechanisms, fostering an easier understanding of the relations between causes and consequences of these interactions and to make predictions. The formalisation through mathematical models able to offer analytical solutions being most often not possible in order to provide satisfying representations of social complexity~\citep{jensen2018pourquoi}, computational models based on agents are more and more used. For long the limited computational capabilities of computer have forbidden to program models taking into account interactions between large numbers of entities geographically localized (individuals or territories). In principle these models should inform on the possibilities and conditions of the emergence of given configurations defined at a macro-geographical level from interactions occurring at a micro-geographical level, within systems with a too much complex behavior to be understood by a human brain. This however requires to study the dynamical behavior of these models including non-linear feedback effects and verify they produce plausible results at all stages of their simulation. This necessary stage of the exploration of the dynamics of algorithms remained rather rudimentary until the end of the last decade, when algorithms including more sophisticated methods such as evolutionary computation and the use of distributed high performance computing have allowed a significant qualitative leap forward in the validation of models, and even an epistemological turn for social sciences and humanities, as suggest the latest applications realized with the OpenMOLE platform described here.
\end{abstract}


%1 Les sciences sociales et l’expérimentation
\section{Social sciences and experimentation}


%L’expérimentation a beaucoup aidé à construire les sciences de la nature, en ce qu’elle consiste à simuler des processus matériels, physiques, chimiques ou biologiques, selon des dispositifs imaginés par les chercheurs pour sélectionner, souvent en les isolant, des enchaînements de faits plus simples que ceux opérant dans une réalité complexe. La confrontation des résultats de ces manipulations à des données d’observation, en totalité ou en partie étrangères à celles qui ont servi à construire le dispositif expérimental, est considérée comme apportant une preuve de la véracité ou de la justesse du raisonnement explicatif qui est à la base de la construction du modèle, plus ou moins probante en fonction de la qualité de l’ajustement entre les prédictions du modèle et les observations. On sait cependant que la justesse des prédictions d’un modèle ne suffit pas à valider totalement l’adéquation entre le mécanisme explicatif imaginé par les manipulateurs du dispositif expérimental et les processus à l’œuvre dans le système étudié, mais c’est une étape importante dans la construction de modèles et de théories enrichis par les observations.
%  miss the "model building" part ? - OK - general
Experimentation played a significant role in the construction of natural sciences, since it consists in simulating material, physical, chemical or biological processes, through the use of apparatus imagined by researchers to select, often by isolating them, chains of facts that are simpler than the ones occurring in a complex reality. The confrontation of results of these experiments to observational data, partly or totally foreign to the data used to construct the experimental apparatus, is considered as bringing a proof of truth or of accuracy of the explicative reasoning at the basis of the model construction, more or less robust depending on the quality of the fit between model predictions and observations. We however know that the accuracy of a model predictions is not sufficient to fully validate the correspondance between the explicative mechanism imagined by the builders of the experimental apparatus and processes at work in the studied system, but this remains a crucial stage in the construction of models and theories enriched by observations.


%En sciences humaines et sociales, la mise au point de dispositifs expérimentaux est très délicate car elle se heurte à de nombreux obstacles  pratiques et éthiques. La critique éthique et politique met en question la manipulation des personnes et l’usurpation de leur liberté. Ces scrupules propres à l’ontologie et la déontologie scientifiques (faisant partie de ce qu’on appelle aujourd’hui l’intégrité) n’ont certes pas empêché en pratique les manipulations, bienveillantes ou non, opérées au cours des temps historiques par des acteurs disposant d’un pouvoir politique, culturel ou économique de prendre des décisions, plus ou moins bien informées « scientifiquement » (voir à toutes époques les écrits des « conseillers du prince », tels Bodin, Machiavel, Botero… pour citer quelques-uns parmi ceux qui ont traité de l’aménagement des territoires) et de procéder à des « expérimentations » de formes de gouvernance ou d’innovations technologiques ou culturelles dont les résultats ont pu être évalués, tantôt comme bénéfiques et tantôt comme désastreux. L’évaluation de l’efficacité des décisions se complique du fait des justifications qu’apportent les acteurs eux-mêmes avec leurs « prophéties auto-réalisatrices » (Rist, 1970). La difficulté souvent déplorée de l’évaluation des politiques publiques est aussi accrue par l’incertitude des limites entre l’action et son contexte, dans l’espace comme dans le temps. 
In social sciences and humanities, the elaboration of experimental apparatus is highly problematic since it is confronted to numerous practical and ethical obstacles. Ethical and political critic questions the manipulation of individuals and the usurpation of their freedom. These concerns which are typical of the scientific ontology and deontology (being part of what is nowadays called integrity) have surely not avoided in practice manipulations, in a positive way or not, operated during historical times by actors with a political, cultural or economical power to make decisions which were more or less well informed ``scientifically'' (see at all historical periods writings by ``counsellors of the prince'' such as Bodin, Machiavel, Botero, etc. to give a few among the ones having dealt with the planning of territories) and to proceed to ``experiments'' of governance structures or of technological or cultural innovations which results could be evaluated in some case as beneficial and in others as catastrophic. The evaluation of the efficacy of decisions complicates because of the justifications brought by the actors themselves with their ``self-fulfilling prophecies''~\citep{rist1970student}. The often recalled difficulty of the evaluation of public policies is also increased by the uncertainty in the limits between the action and its context, both in space and in time.


%Conduire le changement dans la vie sociale, quelle que soit l’échelle des interventions, reste une opération coûteuse et risquée, donc déontologiquement peu acceptable par les scientifiques, dont bien peu finalement osent se lancer dans des projets de « recherche-action ». Une controverse a ainsi opposé pendant les années 1960 en France les tenants d’une « géographie appliquée », bonne connaisseuse du « terrain » mais parfois de tendance conservatrice à ceux d’une « géographie active » plus engagée dans la transformation de la société. Parfois, par exemple pour contribuer à la définition de la politique des métropoles d’équilibre en France (opération de la DATAR1 en 1964), les géographes participant aux études (dont Michel Rochefort en l’occurrence) s’appuyaient, sans vraiment oser le dire, sur des modèles scientifiques (dans ce cas précis la théorie des lieux centraux de Walter Christaller). Les géographes actuels affichent plus volontiers un souci d’aider à la décision de la manière la plus avisée possible selon l’état de leurs connaissances, ils font alors souvent le choix de recourir à des modèles de simulation, in silico, opérés par des ordinateurs. La simulation informatique est ainsi devenue un substitut à l’expérimentation. Ce n’est pas un hasard si, parmi les chercheurs en sciences sociales, les géographes s’y sont intéressés très tôt : la diversité des multiples sources de données (paysages, populations, habitats…) qu’ils manipulent pour rendre compte des aménagements apportés par les sociétés aux interfaces terrestres, l’étendue souvent large des territoires qu’ils examinent aux échelles régionale, nationale ou mondiale, expliquent leur besoin de recourir au calcul pour organiser ces masses d’information et comprendre les dynamiques qu’elles représentent.
Driving change in social systems, whatever the scale of interventions, remains a costly and risky operation, therefore difficultly acceptable by science for deontological concerns. Very few scientists therefore engage in ``research-action'' projects. A controversy has thus opposed in the sixties in France the advocates of an ``applied geography'' with a good knowledge of the ``field'' but sometimes with conservative trends, to the defenders of an ``active geography'' which would be more implied in the transformation of society. Sometimes, for example to contribute to the definition of policies for balancing metropolitan areas in France (operation by the \emph{D{\'e}l{\'e}gation à l'Am{\'e}nagement du Territoire et à l'Action R{\'e}gionale} in 1964), geographers participating in the studies such as Michel Rochefort more particularly, did rely on scientific works, without having the courage to make it explicitly open (in this specific case central place theory by Walter Christaller). Contemporary geographers are less reluctant to exhibit a concern to help decision making in the most informed way possible given the state of their knowledge. They often then make the choice to use simulation models operated in silico by computers. Computer simulation thus became a substitute to experimentation. It is not a coincidence if among researchers in social sciences, geographers have very early found an interest in it: the diversity of multiple data sources (landscapes, populations, built environment, etc.) which they use to account of modifications of terrestrial interfaces by societies, the often large spatial extent of territories they study at the regional, national, or global scales, explain their need to make use of computing to organize this large quantities of information and to understand the dynamics they represent.



%2 Données de la géographie et capacités informatiques
\section{Geographical data and computational capabilities}


%Les premiers modèles de simulation en géographie ont d’abord été calculés « à la main », dans les années 1950. Ce n’est pas un hasard si ces modèles traitent tous de faits stylisés qui traduisent les régularités les plus fréquemment observées dans l’organisation de l’espace social, et qui sont des effets de la « première loi de la géographie » résumée ainsi dès 1970 par le cartographe américain Waldo Tobler : « tout interagit avec tout, mais deux choses proches ont plus de chances d’entrer en contact que deux choses éloignées ». La puissance de l’attrait pour la proximité apparaît dans tous les processus sociaux d’aménagement de l’espace social, qui sont contraints par «  l’obligation d’espacement ». Cette expression a été forgée par Henri Reymond dès 1971 dans une formalisation des problématiques de la géographie, qui posait en premier principe que les sociétés tendent à transformer l’étendue terrestre, hétérogène, rugueuse et discontinue, en espace organisé présentant des propriétés de plus grande homogénéité et continuité, et faisant émerger des régularités, du fait que deux objets ne peuvent occuper la même place. Dire que les personnes et les sociétés ont la probabilité la plus grande de choisir d’occuper les localisations les plus proches, à la fois parce qu’elles sont mieux connues et parce qu’elles permettent de réaliser des économies sur les coûts (physiques, monétaires et culturels) de franchissement de la distance, est certainement la plus forte proposition théorique de la géographie. Elle se repère dans toutes les configurations spatiales conduisant à distinguer un centre et une périphérie, qui se manifestent à tous les échelons de l’espace géographique, du local au mondial. 
%  repet social - ok
The first simulation models in geography were firstly computed ``by hand'' in the fifties. It is not a coincidence if these models all deal with stylized facts which translate the regularities most frequently observed in the organisation of social space, and which are consequences of the ``first law of geography'' summarized as such already in 1970 by the American geographer Waldo Tobler: ``everything interacts with everything, but two closer things have more chances to make contact than two more distant things''. The power of attraction by proximity occurs in all social processes transforming the social space, which are constrained by an ``obligation of space''. This term was forged by Henri Reymond already in 1971 in a formalisation of issues in geography, who stated as first principle that societies have the tendency to transform the surface of the Earth which is heterogenous, rough and discontinuous, in an organized space exhibiting higher homogeneity and continuity, and making regularities emerge, due to the fact that two objects can not occupy the same place. Stating that individuals and societies have the highest probability to choose occupying the closest locations, both because these are better known and also because they yield economies on costs (physical, financial, and cultural) to travel the distance, may certainly be the strongest theoretical proposal of geography. It can be identified in any spatial configuration implying to distinguish a center and a periphery, which are observed at any level of the geographical space, from the local to the global. 


%Les premiers modèles de simulation en géographie ont donc touché d’abord à des processus pour lesquels le choix du plus proche, parmi les lieux avec lesquels on souhaite établir une interaction, est une constante anthropologique très dominante, que ce soit pour observer les effets d’une innovation avant de l’imiter, selon la théorie spatiale de diffusion des innovations de Torsten Hägerstrand (1952) ou encore pour le choix des lieux de destination d’une migration (Hägerstrand 1957, Morrill 1962 et 1963). Les modèles s’appuyant sur la proposition, dès 1954, du géographe américain Edward Ullman de construire une géographie comme la science des interactions spatiales, notamment à propos des relations commerciales, ont d’abord surtout donné lieu à des expérimentations de modèles statistiques, sous l’appellation de « modèle gravitaire », avant d’être intégrés dans des modèles urbains, d’abord statiques (Lowry, 1964) puis dynamiques (Clarke Wilson 1983, Wilson 2014, Allen Sanglier 1981 et Allen 2012). 
% cut phrase trade gravity - OK
% rq : repet si pas autre mot, pas experimental instead of empirical, ≠
The first simulation models in geography have thus dealt with processes for which the choice of the closest, among the place with which an interaction is expected, is a highly salient anthropological constant, either to observe the effects of an innovation before imitating it, according to the spatial theory of the diffusion of innovations by~\cite{hagerstrand1957migration}, or for the choice of destination places for a migration \citep{hagerstrand1957migration,morrill1962development,morrill1963distribution}. Models already in 1954 rely on the proposal by the American geographer Edward Ullman to construct a geography as the science of spatial interactions. This concerns more particularly trade relations, which have first lead to the empirical test of statistical models, as the so-called ``gravity model'', before being integrated into urban models which were first static \citep{lowry1964model} and then dynamical \citep{clarke1983dynamics,wilson2014complex,allen1981urban,allen2012cities}.


%Une génération postérieure de modèles jouant de manière plus complexe avec les effets de la proximité a beaucoup utilisé les automates cellulaires. Les mesures de l’auto-corrélation spatiale, qui traduisent de manière positive ou négative les effets d’attraction ou de concurrence liés à la proximité sont ainsi employées pour tester la plausibilité des configurations simulées pour les changements d’utilisation du sol, et en particulier la croissance urbaine (White Engelen, 1993, White et al. 2015) ou encore la propagation des épidémies dans l’espace géographique (Cliff et al. 2004 )
A later generation of models playing in a more complex way with effects of proximity has intensively used cellular automatons. Measures of spatial auto-correlation, which translate in a positive or negative way attraction or concurrency effects linked to proximity are in that context used to test the plausibility of simulated configurations for land-use changes, and in particular urban growth \citep{white1993cellular,white2015modeling}, or moreover the spread of epidemics in the geographical space \citep{cliff2004world}.


%Mais le développement de ces modèles s’est heurté très tôt à l’obstacle des capacités de calcul des ordinateurs de l’époque, car la représentation explicite des interactions spatiales augmente comme le carré du nombre des unités géographiques considérées. Ainsi, le statisticien Christophe Terrier a dû segmenter son programme Mirabelle ((Méthode Informatisée de Recherche et d'Analyse des Bassins par l'Etude des Liaisons Logement-Emploi) traitant les données du recensement de l’INSEE de 1975 avant de pouvoir simuler les découpages en bassins d’emploi des populations résidentes en fonction des navettes domicile-travail entre toutes les 36 000 communes françaises (Terrier, 1980). Notre premier modèle de simulation des interactions entre des villes destiné à reproduire leurs trajectoires démographiques et économiques influencées par les fonctions urbaines sur une période de 2000 ans sur un ordinateur de laboratoire ne pouvait accepter qu’un maximum de 400 villes (Bura et al., 1996, Sanders et al., 1997). La montée en puissance des calculs informatiques a été relativement lente, autorisant la considération d’environ un millier de villes en 2007 avec le modèle Eurosim (Sanders et al. 2007) ou les modèles Simpop2 appliqués par Anne Bretagnolle à l’Europe et aux Etats-Unis (Bretagnolle et al 2010). Surtout, la méthode d’expérimentation avec ces modèles est longtemps restée à un stade artisanal, requérant une grande application dans la modification « à la main » des valeurs des paramètres, qui ne sont que trop rarement directement observables, et qui doivent donc être estimées d’après la plausibilité des dynamiques du modèle. Or, les équations des modèles de dynamique urbaine intègrent des relations non linéaires qui produisent de nombreuses bifurcations, obligeant à des retours fastidieux dans la procédure d’estimation (Sanders et al. 2013). Ce long travail limite le nombre des simulations à partir duquel l’estimation obtenue peut être jugée comme satisfaisante, et surtout, une fois le modèle ainsi calibré, il demeure une assez grande incertitude quant à la qualité des résultats obtenus.
%  imprecision on nonlinearity / bifurcations ? ok
But the development of these models has been very early impeded by the computational capabilities at this time, since the explicit representation of spatial interactions increases as the square of the number of geographical units considered. Therefore, the statistician Christophe Terrier had to segment his Mirabelle program (\textit{Méthode Informatisée de Recherche et d'Analyse des Bassins par l'Etude des Liaisons Logement-Emploi}) processing household survey data provided by INSEE in 1975 before being able to simulate the clustering into employment centers of resident populations as a function of work-residence commuting between all 36,000 French communes \citep{terrier1980mirabelle}. Our first simulation model of interactions between cities aimed at reproducing their demographic and economic trajectories influenced by urban functions on a period of 2000 years could only accept a maximum of 400 cities on a personal computer \citep{bura1996multiagent,sanders1997simpop}. The increase of computational possibilities has been relatively slow, allowing to consider around one thousand cities in 2007 with the Eurosim model \citep{sanders2007artificial} or the Simpop2 models applied by Anne Bretagnolle on Europe and United States \citep{bretagnolle2010simulating}. Furthermore, the experimentation method with these models stayed at an experimental stage for long, requiring am increased attention in the modification ``by hand'' of parameter values, which are only very rarely directly observable, and which thus must be estimated through the plausibility of model dynamics. However, equations for urban dynamics models integrate non-linear relations which are linked to numerous bifurcations, forcing to laborious trial-and-error loops in the estimation procedure \citep{sanders2007artificial}. This consequent work limits the number of simulations from which the estimation obtained can be judged as satisfying, and more importantly once the model is therein calibrated, there remains a relatively high uncertainty regarding the quality of results obtained.


%3 Les simulations de nouvelle génération
\section{A new generation of simulations}


%La fin des années 1990 devait modifier totalement l’environnement de travail des chercheurs, la diffusion d’Internet puis des téléphones portables et enfin des données massives produites par toutes sortes de capteurs numériques ayant des effets en retour rapides et intenses sur la montée en puissance des capacités de calcul informatique qui avaient permis ces innovations technologiques disruptives. Les modèles de simulation peuvent alors intégrer des quantités considérables d’interactions entre des entités localisées caractérisées par une grande diversité d’attributs. Il y a une quinzaine d’années encore, (Gleyze, 2005) était forcé de conclure que les analyses de réseau, pour les transports publics parisiens, étaient “limitées par le calcul”. Pour ne donner qu’un exemple du bond quantitatif représenté par l’accroissement des capacités de calcul et de ses conséquences sur la plus grande confiance accordée aux modèles qui en découle, on peut citer le travail pionnier en épidémiologie numérique réalisé par Eubank et al. (2004) pour simuler au moyen des modèles EpiSims et TRANSIMS les trajectoires quotidiennes, sur un réseau de transport, des déplacements d’un million et demi de personnes entre quelque 180 000 lieux de la ville virtuelle de Portland, afin de prédire les chemins de propagation d’une épidémie à partir des probabilités de rencontre interpersonnelles, dans des réseaux sociaux organisés en « petits mondes ». L’épidémie peut se propager rapidement par toute la ville alors que les nombres de contacts par personne restent petits (quinze au maximum, Eliot et Daudé, 2006). 
The end of the nineties was to modify completely the working environment of researchers, the diffusion of internet and then mobile phones and finally of massive data produced by diverse numerical sensors having in return rapid and intense effects on the increase of computational power which had allowed these disruptive technological innovations. Simulation models can then integrate considerable quantities of interactions between localized entities chracterized by a large diversity of attributes. Still fifteen years ago, \cite{gleyze2005vulnerabilite} was forced to conclude that network analysis in the case of the Parisian transportation network were ``limited by computation''. To give a single example of the quantitative leap forward in the increase of computational capabilities and their consequences on the higher confidence given to the models in consequence, we can mention the pioneering work in numerical epidemiology realized by \cite{eubank2004modelling} to simulate through the EpiSims and TRANSIMS models the daily trajectories on a transportation network of commuting of a million and a half individuals between around 180,000 places in the virtual city of Portland, in order to predict transmission pathways of an epidemics starting from interpersonal meeting probabilities in social networks organized as ``small worlds''. The epidemics can rapidly propagate to the whole city despite the number of contacts by individual remain low (fifteen in maximum \citep{eliot2006diffusion}).



%Des plateformes de simulation sont mises au point pour que le plus grand nombre de chercheurs, même non spécialisés en informatique, puissent mettre en œuvre des modèles multi-agents. Netlogo (Tisue et Wilensky 2004) est sans doute la plus connue, elle est généraliste et permet un accès aux simulations multi-agent sans besoin de connaissance informatiques approfondies, grâce à son langage de programmation simple et le constructeur d’interface graphique intégré. D’autres plateformes plus spécialisées, comme GAMA (Grignard et al., 2013), sont d’emblée construites pour proposer un couplage avec des systèmes d’information géographique. Cependant, la confiance dans les résultats issus des modèles de simulation va de pair avec une augmentation de la taille et du nombre de simulations requises, c’est-à-dire de l’ampleur des expériences numériques. Bien que ces plateformes intègrent des outils de base permettant un premier pas vers un tel passage à l’échelle, un besoin de “méta-plateforme” dédiée a naturellement émergé.
Simulation platforms are elaborated such that the largest number of researchers even not specialized in computer science can elaborate agent-based models. NetLogo \citep{tisue2004netlogo} is amongst the most famous. It is generic and allows to access multi-agent simulations without a deep knowledge in algorithmics, thanks to its simple programming language and the integrated builder of graphic user interface. Other platforms which are more specialized such as GAMA \citep{grignard2013gama} are immediately elaborated to propose a coupling with geographic information systems. However, the confidence in results obtained from simulation models goes along with an increase in the size and the number of experiments required, i.e. of the amplitude of numerical experiments. Despite the fact that these platforms integrate basic tools for a first step towards such a change in scale, a need for a dedicated ``meta-platform'' has naturally emerged.



%3.1 Un  laboratoire virtuel : la plateforme OpenMOLE
\subsection{A virtual laboratory: the OpenMOLE platform}


%Depuis 2008 le logiciel OpenMOLE a été conçu pour explorer la dynamique des modèles multi-agents (Reuillon, Chuffart et al., 2010 ; Reuillon, Leclaire et Rey-Coyrehourcq, 2013). Il est issu du développement d’un précédent logiciel, SimExplorer (Amblard, 2003 ; Deffuant et al., 2003), qui offrait déjà à ses utilisateurs une interface ergonomique pour la conception de plans d’expérimentation et donne accès au calcul distribué. OpenMOLE généralisait dans ses débuts SimExplorer en particulier en rendant possible la parallélisation massive de tâches. OpenMOLE2 est un outil de modélisation collaboratif en perpétuelle évolution : « Un effort permanent de généricité a permis de réaliser en quelques années une plateforme générique, pragmatique et éprouvée pour l’exploration de modèles de systèmes complexes sous forme d’un langage dédié, textuel et graphique, exposant des blocs cohérents et au bon niveau d’abstraction pour la conception d’expérimentations numériques distribuées sur des modèles de simulation » (Schmitt, 2014).
%  not only multi-agent - was it at the beginning ? OK precised.
% - note on difference oml/simexplorer ? (parallelisation of workflows ?) - OK
Since 2008, the OpenMOLE software has been conceived to explore the dynamics of multi-agent models \citep{reuillon2010declarative,reuillon2013openmole}, and rapidly simulation models in general. It inherits from the development of a previous software SimExplorer \citep{amblard2003comprendre,deffuant2003demarche} which already provided to users an ergonomic interface for the conception of experience plans and gave access to distributed computing. OpenMOLE was generalizing SimpExplorer in the beginning in particular by enabling the parallelisation of large workflows. OpenMOLE (\texttt{https://openmole.org/}) is a collaborative modeling tool in constant evolution: ``\textit{a permanent effort for genericity has allowed to realize in a few years a pragmatic, generic, and proofed platform for the exploration of models of complex systems under the form of a dedicated language, both graphical and textual, exposing consistent blocks at the appropriated level of abstraction for the design of numerical experiments distributed on simulation models}'' \citep{schmitt2014modelisation}.


%Les procédures (ou workflow) proposées dans OpenMOLE sont décrites de manière indépendante des modèles et sont donc reproductibles, ré-utilisables et échangeables entre modélisateurs. Une place d’échange (market place) est intégrée au logiciel, à l’image de la librairie de modèles incluse dans NetLogo, et permet aux utilisateurs de bénéficier de scripts d’exploration pouvant servir de template ou d’exemple, dans des domaines thématiques très variés et pour l’ensemble des méthodes et langages implémentés dans OpenMOLE (par exemple pour les champs thématiques calibration de modèles géographiques, analyse de réseaux biologiques, traitement d’images pour les neurosciences).
Procedures (or workflows) proposed in OpenMOLE are described in a manner independent from the models and are thus reproducible, reusable and exchangeable between modelers. A market place is integrated to the software, similarly to the model library included in NetLogo, and allows users to collect exploration scripts that can act as template or example, in highly diverse thematic fields and for all methods and languages implemented in OpenMOLE (for example for the thematic fields calibration of geographical models, analysis of biological networks, image processing for neurosciences).


%Il est utile de mentionner l’utilisation par OpenMOLE d’un Domain Specific Language (DSL) (Van Deursen and Klint, 2002) pour l'écriture des workflow d’exploration. Cette pratique consiste en la création d’une notation et de règles spécifiques au domaine d’un problème donné. Il s’agit en quelque sorte d’un langage de programmation dédié dans ce cas à l’exploration des modèles et aux méthodes associées. Celui-ci n’est bien sur pas créé de toutes pièces, mais vient comme une extension du langage sous-jacent, c’est-à-dire Scala dans le cas d’OpenMOLE. Un nombre réduit de mot-clés et de primitives rend l’utilisation aisée même pour un utilisateur qui n’aurait aucune connaissance en programmation, et par ailleurs le DSL reste très flexible pour l’utilisateur avancé qui peut utiliser la programmation Scala. Selon Passerat-Palmbach et al. (2017), le DSL d’OpenMOLE est l’un des éléments clés de sa généricité et de son accessibilité.
It is useful to mention the use by OpenMOLE of a Domain Specific Language (DSL) \citep{van2002domain} to write exploration workflows. This practice consists in the construction of a notation and rules specific to the domain of a given problem. It is in a way a programmation language dedicated in that case to model exploration and associated methods. This language is naturally not created from scratch, but comes as an extension of the underlying language, i.e. the Scala language in the case of OpenMOLE. A reduced number of keywords and primitives fosters an easier use even for a user with no knowledge in programming, and furthermore the DSL remains highly flexible for the advanced user who can use Scala programming. According to \cite{passerat2017reproducible}, the DSL of OpenMOLE is one of the key elements of its genericity and accessibility.



%Notons également que l’un des atouts principaux d’OpenMOLE est l'accès transparent aux environnements de calcul haute performance (HPC). L’augmentation des moyens de calcul mentionnée précédemment peut se manifester physiquement sous différents aspects pour le modélisateur:  serveur local, cluster de calcul local, grille de calcul (mise en réseau de multiples clusters, comme la grille de calcul européenne EGI), services de cloud computing. Leur utilisation demande dans la majorité des cas des compétences informatiques avancées, généralement inaccessibles à un modélisateur géographe standard. OpenMOLE intègre une bibliothèque permettant d'accéder à la majorité de ces moyens de calcul, et leur mobilisation dans le DSL est entièrement transparente pour l’utilisateur. Celui-ci peut tester son script sur sa propre machine et passer à l'échelle sur les environnements HPC en modifiant un seul mot-clé dans celui-ci.
We can also remark that one of the main assets of OpenMOLE is the transparent access to High Performance Computing environments (HPC). The increase in computational capabilities already described can in practice be implemented physically under different forms for the modeler: local server, local computation cluster, computation grid (network of multiple clusters, such as the European computing grid EGI), cloud computing services. Their use requires in most cases advanced computer science knowledge which are generally inaccessible to the standard modeler in geography. OpenMOLE integrates a library allowing to access most of these computing facilities, and their integration in the DSL is totally transparent for the user. The user script can then be tested on the local machine and then scaled on the HPC environments by modifying a single keyword in it. 


%La présentation de l’utilisation du DSL et de la mise en place de scripts n'étant pas l’objectif de ce chapitre, nous renvoyons le lecteur à la documentation en ligne d’OpenMOLE pour des exemples de scripts et d’exploration de modèles. Nous rappelons simplement les composants fondamentaux d’un script d’exploration: (i) la définition de prototypes, qui correspondent aux paramètres et aux sorties du modèles, qui prendront différentes valeurs lors de l'expérience; (ii) la définition de tâches, incluant l'exécution du modèle mais pouvant aussi par exemple être des pré- ou post-traitements - les tâches couvrant une très grande variété de langages (scala, java, NetLogo, R, Scilab, code natif comme python ou C++); (iii) la description des méthodes à appliquer (exploration par échantillonnage, calibrage, recherche de diversité, etc.) qui agiront sur les valeurs des prototypes et lanceront la tâche d'évaluation considérée (le plus souvent le modèle); (iv) une spécification des données recupérées en sortie de l’exécution du script (les données de simulation étant souvent massives, une sélection par cette étape est cruciale); et (v) la définition de l’environnement de calcul sur lequel la méthode sera lancée.
The presentation of how to use the DSL and to elaborate scripts is out of the scope of this chapter, and we refer the reader to the online documentation of OpenMOLE for examples of scripts and model explorations. We simply recall the fundamental components of an exploration script: (i) the definition of prototypes, which correspond to parameters and outputs of the model, and which will take different values during the experiment; (ii) the definition of tasks, including model execution but that can also be for exemple pre- or post-processing tasks - the tasks covering a high variety of languages (scala, java, NetLogo, R, Scilab, native code such as python or C++); (iii) the description of methods to be applied (exploration by sampling, calibration, diversity search, etc.) which will act on the values of prototypes and will launch the considered evaluation task (mostly the model); (iv) a specification of the data gathered as an output of script execution (simulation data being often massive, a selection through this stage is crucial); and (v) the definition of the computation environment on which the method will be launched.


%La plateforme vise à considérablement étendre les pratiques de la generative social science proposée par Epstein et Axtell (1996), qui envisageait chaque modèle multi-agents comme une société artificielle, engendrant des comportements macroscopiques à partir d’hypothèses émises sur les comportements microscopiques. Les expériences numériques envisageables changent d'échelle, et les questions posées au modèle de nature qualitative. Selon Clara Schmitt (2014), qui a utilisé la plateforme OpenMOLE pour développer avec Sébastien Rey Coyrehourcq (2014) le modèle SimpopLocal destiné à simuler l’émergence d’un système de villes, le laboratoire virtuel que représente cette plateforme « n’est plus seulement le modèle de simulation et les hypothèses qu’il simule (i.e. l’artificial society). Il contient aussi les méthodes, les outils et procédures de modélisation adaptés à la conception et à l’exploration du modèle et dont la pratique procure autant de connaissances et de retours théoriques que la conception du modèle lui-même. Ce laboratoire virtuel s’apparente donc d’autant plus à un véritable laboratoire de recherche avec une paillasse (le modèle à concevoir et explorer), les hypothèses d’un chercheur (les processus géographiques transcrits en mécanismes du modèles), des méthodes (la méthode de modélisation itérative et assistée par le calcul intensif), des outils (les procédures d’exploration automatisées et tout autre plan d’expérimentations incorporé dans OpenMOLE), le tout rassemblé dans une salle, la plateforme de modélisation SimProcess [une appellation alternative du cadre dans lequel s'inscrit OpenMOLE] (Rey Coyrehourcq, 2014) ».
% -  unclear what simprocess is at this stage - ok
The platform aims at considerably extending practices of generative social science proposed by \cite{epstein1996growing}, which considers each multi-agent model as an artificial society, yielding macroscopic behaviors from assumptions made on microscopic behaviors. Numerical experiments that can be considered follow a change in scale, and the questions asked to the model evolve in a qualitative way. According to \cite{schmitt2014modelisation}, who used the OpenMOLE platform to develop with \cite{rey2015plateforme} the SimpopLocal model aimed at simulating the emergence of a system of cities, the virtual laboratory represented by this platform ``\textit{is not anymore only the simulation model and the hypothesis it simulated (i.e. the artificial society). It also contains the methods, tools and modeling procedures adapted to the conception and the exploration of the model and which practice creates as much knowledge and theoretical feedbacks than the conception of the model itself. This virtual laboratory is thus furthermore resembling a real research laboratory with an experimental desk (the model to conceive and explore), the assumptions of a researcher (the geographical processes translated into model mechanisms), methods (the iterative modeling method and aided by intensive computation), tools (the procedure for automatic exploration and any other experience plan integrated in OpenMOLE), all this gathered in a single room, the modeling platform SimProcess [an alternative name for the framework in which OpenMOLE in inserted] \citep{rey2015plateforme}}''.


%Par rapport aux protocoles généraux comme celui introduit par (Grimm et al., 2014) pour présenter l’ensemble des étapes de la modélisation, les principes appliqués dans OpenMOLE apportent surtout de la nouveauté en termes de capacités inédites d’exploration du comportement dynamique des modèles de simulation. Deux principales innovations consistent dans l’emploi systématique de meta-heuristiques d’optimisation, principalement des algorithmes génétiques, pour tester rapidement le plus grand nombre possible de combinaisons de valeurs des paramètres du modèle, et dans l’envoi simultané des simulations sur les multiples machines d’une grille de calcul, ce qui permet de réduire considérablement la durée des expériences qui sans cela deviendrait rapidement prohibitive.
In comparison to general protocols as the one introduced by \cite{grimm2005pattern} to describe all the stage of the modeling process, principles applied in OpenMOLE mostly innovate regarding the potentialities without precedent to explore the dynamical behavior of simulation models. Two main innovations rely in the systematic application of optimization meta-heuristics, mainly genetic algorithms, to rapidly test the largest possible number of combinations for model parameter values, and in the simultaneous distribution of simulations on multiple machines of a computation grid, what allows to considerably reduce the length of experiments without which it would become quickly prohibitive.


%Le choix des algorithmes génétiques comme heuristique d’optimisation est justifié par leur efficacité dans le cadre de problèmes d’optimisation multi-objectifs. Par ailleurs, le schéma de distribution en îles (populations évoluant indépendamment pendant une certaine durée) est particulièrement adapté à la distribution sur grille, chacun des noeuds faisant évoluer une sous-population, qui est régulièrement récupérée, fusionnée dans la population globale, à partir de laquelle une nouvelle sous-population est générée et envoyée sur le noeud. Ce type d’algorithme s'étend par ailleurs relativement bien aux modèles stochastiques, même si cet aspect comporte encore un certain nombre de problèmes ouverts (Rakshit, Konar and Das, 2017). Suivant Rey-Coyrehourcq (2015), ces méthodes se situent dans le cadre plus global de l'Evolutionary Computation, et la bibliothèque scala MGO développée simultanément à la plateforme et qui permet d’y implémenter les algorithmes évolutionnaires, a été conçue pour être facilement étendue à d’autres heuristiques en Evolutionary Computation, laissant les possibilités de méthodes incluses dans OpenMOLE totalement ouvertes.
The choice of genetic algorithms as an optimization heuristic is justified by their efficiency in the context of multi-objective optimization problems. Moreover, the island distribution scheme (populations evolving independantly during a given duration) is particularly suited to a distribution on grid, each node making a subpopulation evolve, which is regularly fetched, merged into the global population, and from which a new subpopulation is generated and sent on the node. This type of algorithms furthermore extends relatively well to stochastic models, even if this aspect still implies a certain number of open problems \citep{rakshit2017noisy}. Following \cite{rey2015plateforme}, these methods are situated within the larger context of Evolutionary Computation, and the scala library MGO developed simultaneously to the platform and which allows to implement evolutionary algorithms in it, has been conceived to be easily extended to other heuristics in Evolutionary Computation, opening totally the possibilities for the inclusion of new methods in OpenMOLE.


%(Reuillon, Leclaire et Rey-Coyrehourcq, 2013) décrivent les principes fondamentaux de la plateforme, tandis que (Pumain et Reuillon, 2017) donnent une contextualisation des différentes utilisations dans le cadre de modèles de simulation pour les systèmes de villes. Selon R. Reuillon cité par Raimbault (2017a), la philosophie d’OpenMole s’articule autour de trois axes: le modèle comme “boîte noire” à explorer (i.e. méthodes indépendantes du modèle), l’utilisation de méthodes avancées d’exploration, et l’accès transparent aux environnements de calcul intensif. Ces différentes composantes sont en interdépendance forte, et permettent un changement de paradigme dans l’utilisation des modèles de simulation : utilisation de multi-modélisation, c’est-à-dire structure variable du modèle, comme il a été présenté au chapitre 4 (Cottineau et al., 2015), changement de la nature des questions posées au modèle (par exemple détermination complète de l’espace faisable (Chérel, Cottineau et Reuillon, 2015)), tout cela permis par l’utilisation du calcul intensif (Schmitt et al., 2015). Les différentes méthodes disponibles dans ce cadre seront illustrées ci-dessous dans des exemples concrets. La documentation en ligne donne un aperçu global des méthodes disponibles dans la version la plus récente du logiciel et de leur articulation dans un cadre standard.
%  repetitions ? ok leger
\cite{reuillon2013openmole} describe the fundamental principles of the platform, whereas \cite{pumain2017urban} give a contextualisation of the different uses in the frame of simulation models for systems of cities. According to R. Reuillon cited by \cite{raimbault2017applied}, the philosophy of OpenMOLE is articulated around three axis: the model as a ``black box'' to be explored (i.e. methods which are independent from the model), the use of advanced exploration methods, and the transparent access to intensive computation environments. These different components are in a strong interdependence, and allow a paradigm shift in the use of simulation models: use of multi-modeling, i.e. variable structure of the model such as it was presented in chapter 4 \citep{cottineau2015modular}, change in the nature of questions asked to the model (for example full determination of the feasible space \citep{10.1371/journal.pone.0138212}), all this allowed by the use of intensive computation \citep{schmitt2015half}. The different methods available in that context will be illustrated below in concrete examples. The online documentation gives a broad overview of the available methods in the most recent version of the software and of their articulation within a standard context.


%Nous considérons un modèle de simulation comme un algorithme produisant des sorties à partir de données et de paramètres en entrée. Dans ce cadre, nous rappelons que dans un cas idéal, l’ensemble des étapes suivantes devraient être nécessaires pour une utilisation robuste des modèles de simulation.
We consider a simulation model as an algorithm producing outputs from data and parameters as inputs. In that frame, we recall that in an ideal case, all the following stages should be necessary for a robust use of simulation models.

\begin{enumerate}
	% Identification des mécanismes principaux et des paramètres cruciaux associés, ainsi que de leur domaine de variation suggéré par leur signification thématique le cas échéant ; identification des indicateurs pour évaluer la performance ou le comportement du modèle.
	\item Identification of principal mechanisms and of crucial associated parameters, also with their variation range suggested by their thematic signification if they have some; identification of indicators to evaluate the performance or the behavior of the model.
	% Évaluation des variations stochastiques : grand nombre de répétitions pour un nombre raisonnable de paramètres, établissement du nombre de répétitions nécessaire pour atteindre un certain niveau de convergence statistique.
	\item Evaluation of stochastic variations: large number of repetitions for a reasonable number of parameters, assessment of the number of necessary repetitions to reach a certain level of statistical convergence.
	% Exploration directe pour une première analyse de sensibilité, si possible évaluation statistique des relations entre paramètres et indicateurs de sortie.
	\item Direct exploration for a first sensitivity analysis, if possible statistical evaluation of relations between parameters and output indicators.
	% Calibrage, exploration algorithmique ciblée par l’utilisation d’algorithmes spécifiques (Calibration Profile (Reuillon, Schmitt at al. 2015), Pattern Space Exploration (Chérel, Cottineau et Reuillon 2015)).
	\item Calibration, targeted algorithmic exploration through the use of specific algorithms (Calibration Profile \citep{reuillon2015new}, Pattern Space Exploration \citep{10.1371/journal.pone.0138212}).
	% Retours sur le modèle, extension et nouvelles briques de multi-modélisation, retours sur les faits stylisés et la théorie. 
	\item Feedbacks on the model, extension and new multi-modeling bricks, feedbacks on stylized facts and theory.
	% Analyses de sensibilité étendues, correspondant à des méthodes expérimentales en cours d'élaboration et d'intégration dans la plateforme, comme par exemple la sensibilité aux méta-paramètres et aux conditions spatiales initiales proposée par (Raimbault et al. 2018).
	\item Extended sensitivity analysis, corresponding to experimental methods currently in construction and integration into the platform, such as for example the sensitivity to meta-parameters and to initial spatial conditions proposed by \cite{raimbault2018space}.
\end{enumerate}

%Le cas échéant, certaines étapes n’ont pas lieu d’être, par exemple l’évaluation de la stochasticité dans le cas d’un modèle déterministe. De même, les étapes prendront plus ou moins d’importance selon la nature de la question posée : le calibrage ne sera pas pertinent dans le cas de modèles complètement synthétiques, tandis qu’une exploration systématique d’un grand nombre de paramètres ne sera pas forcément nécessaire dans le cas d’un modèle qui a pour but d’être calibré sur des données.
In some cases, some stages to not necessary take place, for example the evaluation of stochasticity in the case of a deterministic model. Similarly, each step take more or less importance depending on the nature of the question asked: calibration will not be relevant in the case of fully synthetic models, whereas a systematic exploration of a large number of parameters will not be necessary in the case of a model aimed at being calibrated on data.

%Afin de mieux illustrer cette présentation générale de la plateforme et des méthodes associées, nous proposons dans la suite de cette section de développer précisément l’exemple du modèle SimpopLocal, dont la genèse a étroitement été liée à celle de la plateforme, et qui a été candidat pour le développement et l’application de diverses méthodes.
In order to better illustrate this general presentation of the platform and associated methods, we propose in the following of this section to precisely develop the example of the SimpopLocal model, which genesis has been tightly linked to the one of the platform, and which has been candidate for the development and the application of diverse methods.


%3.2 L’expérience Simpoplocal: simulation d’une émergence en géographie
\subsection{The SimpopLocal experiment: simulation of an emergence in geography}


%Le modèle SimpopLocal a été conçu pour représenter l’émergence des systèmes de villes, telle qu’on a pu l’observer dans cinq ou six régions du monde, quelque 3000 ans après l’émergence de pratiques agricoles dans des sociétés sédentarisées (Bairoch 1985; Marcus et al. 2008). Il s’agit bien d’expliquer l’émergence, non pas seulement de “la” ville, mais bien de “systèmes de villes”, car on sait que les villes dès cette époque n’étaient jamais isolées mais déjà organisées en réseaux dans le territoire de chacune de ces “civilisations” antiques. Les publications les plus récentes des archéologues insistent sur une certaine continuité des processus ayant conduit à la sédentarisation de populations de chasseurs-cueilleurs, regroupées en hameaux et villages, puis à l’apparition de villes dans certaines de ces régions. Le développement de l’agriculture a été concomitant d’un accroissement considérable des densités de population et de la taille des groupes humains dans ces contrées (on passe de 0,1 personne par km2 à 10, soit un facteur 100 entre les deux ordres de grandeur), ainsi que d’une complexification de l’organisation politique et de la division sociale du travail. Ce processus très lent d’accumulation des ressources et de concentration des populations s’effectue selon des enchaînements comportant de nombreux feedbacks, avec beaucoup de fluctuations dans la croissance, dues aux fréquents événements contraires que sont les catastrophes naturelles ou les prédations de groupes voisins. En raison de la lenteur des transformations et de leurs fréquentes interruptions, les archéologues contestent parfois désormais l’appellation de “révolution néolithique” qui avait été proposée en 1942 par Gordon Childe (Demoule 2018 p.159). 
The SimpopLocal model has been conceived to represent emergence of systems of cities such as it has been observed in five or six regions of the world around 3000 years after the emergence of agriculture practices in sedentary societies \citep{marcus2008ancient}. The purpose is to explain the emergence not only of ``the'' city but indeed of ``systems of cities'' since we know that cities already at this time were never isolated but already organized as networks in the territory of each of these antic ``civilisations''. The most recent publications by archeologists insist in a certain continuity of processes which led to the settlement of hunter-gatherer populations, gathered in hamlets and villages and then to the apparition of cities in some of these regions. The development of agriculture has been concomitant to a considerable increase of population densities and of the size of human groups in these countries (the density switch from 0.1 person per square kilometre to to, i.e. a factor of 100 between the two orders of magnitude), ans also a complexification of the political organization and of the social division of labor. This very slow process of accumulation of ressources and of concentration of populations is done through chains including numerous feedbacks with many fluctuations in growth, due to the frequent adversary events that are natural catastrophes and the predations of neighbor groups. Because of the slow rate of transformations and their frequent interruptions, archeologists now sometimes contest the name of ``neolithic revolution'' which was proposed in 1942 by Gordon Childe \citep[p. 159]{demoule2018histoire}.


%Cependant, les géographes continuent à identifier l’apparition des villes comme une émergence, une “bifurcation” pour deux raisons principales: d’une part elle ne s’est pas produite systématiquement dans toutes les régions où l’agriculture a été pratiquée, donc deux régimes d’évolution des systèmes de peuplement sont possibles et viables historiquement (des régions seulement agricoles et villageoises ont pu fonctionner pendant plusieurs siècles et subsistent aujourd’hui de façon résiduelle dans certaines forêts ou sur des îles du Pacifique par exemple), donc le régime territorial fonctionnant avec des villes constitue bien un “attracteur” spécifique dans la dynamique des systèmes de peuplement anciens; d’autre part, la trajectoire évolutive qui voit naître les villes traduit un changement qualitatif important (une émergence) avec un accroissement significatif de la diversité des fonctions sociales associées aux habitats et aussi un élargissement considérable dans l’échelle de la vie de relations: les échanges commerciaux qui s’y effectuent à plus longue distance permettent ainsi aux villes d’être moins dépendantes d’un “site” de ressources locales comme le sont les villages agricoles et de développer les atouts d’une “situation” géographique exploitant les richesses d’un réseau de sites de plus en plus lointains (Reymond, 1971). 
However, geographers still identify the apparition of cities as an emergence, or a ``bifurcation'' for two main reasons: first it did not systematically occur in all regions where agriculture was practiced, therefore two evolution regimes for settlements systems are possible and historically viable (agricultural only regions and with villages may have functioned for centuries and still exist nowadays in a residual way in some forests and on pacific islands for example), therefore the territorial regime including cities indeed constitutes a specific ``attractor'' in the dynamic of ancient settlements systems; secondly the evolutive trajectory which led to the birth of cities translates an important qualitative change (an emergence) with a significant increase of the diversity of social functions associated to habitats and also a considerable broadening in the scale of the life of relations: commercial exchanges which are done there at a more or less high distance allow thus the cities to be less dependent of a ``site'' of local resources as are agricultural villages, and to develop the assets of a geographical ``situation'' exploiting the wealth of a network of sites more and more distant \citep{reymond1971pour}.



%Le modèle SimpopLocal vise à reproduire cet aspect remarquable de la dynamique des systèmes de peuplement, qui produit invariablement une amplification de la différenciation hiérarchique entre les habitats, définie dans la littérature comme un fait stylisé majeur: déjà dans tout système, en tout lieu et à tout moment de l’histoire ou de la préhistoire, la répartition des tailles des lieux habités (mesurée par la population ou l’étendue spatiale, voire la diversité des artefacts fonctionnels) est statistiquement très dissymétrique, comportant de nombreuses très petites agglomérations et seulement quelques très grandes agglomérations selon une distribution assez régulière de type loi de Zipf ou log normale (Fletcher 1986; Liu 1996). Ce schéma hiérarchique est une propriété structurelle (ordre de taille des entités) au niveau macroscopique particulièrement persistante dans le temps, quelles que soient les fluctuations locales intervenant au niveau des entités. Le modèle SimpopLocal est conçu pour tester l'hypothèse énoncée dans la théorie évolutive des systèmes urbains (Pumain 1997), qui explique cette caractéristique structurelle par un processus de croissance urbaine en moyenne proportionnel à la taille acquise, et son amplification par la création de multiples innovations technologiques et sociétales produisant l’accroissement et la diversification des richesses qui se diffusent parmi les lieux mis en relation par toutes sortes d’échanges.
The SimpopLocal model aims at reproducing this remarkable aspect of the dynamic of settlement systems, which invariably produces an amplification of the hierarchical differentiation between habitats defined in the literature as a major stylized fact: already in any system at any place and at any time of history or prehistory, the size distribution of inhabited places (measured by population or spatial extent, or even the diversity of functional artefacts) is statistically highly dissymmetrical, including numerous very small agglomerations and only few very large agglomerations following a relatively regular distribution of the type of a Zipf law or log-normal law \citep{fletcher1986settlement,liu1996settlement}. This hierarchical schema is a structural property (magnitude of the size of entities) at the macroscopic level which is relatively persistent in time whatever the local fluctuations intervening at the level of entities. The SimpopLocal model is conceived to test the hypothesis introduced in the evolutive theory of urban systems \citep{pumain1997pour}, which explains this structural characteristic by an urban growth process in average proportional to the size attained, and its amplification through the creation of multiple technological and social innovations inducing the growth and the diversification of wealth which diffuse among the places put in relation by any sort of exchanges.

%Le modèle SimpopLocal s’inspire d’abord du modèle statistique qui constitue une excellente première approximation de l’évolution des populations dans un système de villes, en simulant la croissance urbaine comme un simple processus stochastique faisant varier la taille de chaque ville de façon proportionnelle à sa taille et conduisant à une distribution lognormale des populations urbaines (Gibrat 1931). La grande qualité de ce modèle statistique élémentaire tient à ce qu’il utilise comme “explication” de la croissance la taille déjà acquise, laquelle exprime à la fois la richesse accumulée et la capacité d’attraction et de résilience du lieu habité (en quelque sorte il s’agit d’un modèle selon le concept de “croissance endogène” des économistes). Mais SimpopLocal est conçu, comme les modèles précédents de la “famille” des modèles multi-agents Simpop (Bura et al 1996, Sanders et al. 2007), pour pallier l’insuffisante capacité du modèle de Gibrat à prévoir la tendance partout observée à la croissance plus forte qu’attendu des plus grandes villes situées en tête des réseaux (Moriconi-Ebrard, 1993) et à l’exagération de l’inégalité entre les tailles des villes (Pumain 1997, Bretagnolle, Pumain 2010). Ces déviations au modèle de Gibrat sont liées aux corrélations de longue portée (Rozenfeld et al. 2008), suscitées par les interactions spatiales. L’effet de celles-ci amplifie la différenciation hiérarchique entre les tailles des villes participant aux échanges dans un système urbain (Favaro et al. 2011). Les modèles Simpop traduisent cet effet en introduisant, de manière exogène au modèle et à différents moments du temps de la simulation, de nouvelles fonctions urbaines qui sélectionnent certaines villes ou sont captées par elles dans un processus continu d’adaptation à ces innovations. En comparaison des autres modèles Simpop, SimpopLocal introduit deux nouveautés: il utilise une représentation abstraite des vagues d’innovation successives et les rassemble toutes dans un seul objet “innovation”. Une seconde originalité consiste à rendre le processus de création d’innovation endogène en le liant à la taille du lieu habité, censée amplifier de manière non linéaire l’émergence de nouvelles formes techniques, sociales ou culturelles (avec une probabilité de création variant comme le carré des populations en présence ou en relations). Cette version plus parcimonieuse de la construction du modèle permet de réduire considérablement le nombre de paramètres et autorise donc une exploration et une évaluation plus systématiques. 
The SimpopLocal model is first inspired by the statistical model which constitutes a first excellent approximation of the evolution of populations in a system of cities, by simulating urban growth as a simple stochastic process which makes the size of each city vary in a way proportional to its size and leads to a lognormal distribution of urban populations \citep{gibrat1931inegalites}. The high quality of this elementary statistical model relies in the fact that it uses as ``explanation'' of growth the already acquired size, which expresses both the accumulated wealth and the attraction and resilience capabilities of the inhabited place (in some sense this corresponds to a model following the concept of ``endogenous growth'' proposed by economists \citep{aghion1998endogenous}). But SimpopLocal is conceived, as the preceding models of the ``family'' of multi-agent Simpop models \citep{bura1996multiagent,sanders2007artificial}, to palliate to the incapacity of the Gibrat model to predict the trend of a growth larger than expected observed everywhere for largest cities located in central places of networks \citep{moriconi1993urbanisation} and the exaggeration of the inequality between city sizes \citep{pumain1997pour,bretagnolle2010simulating}. These deviations to the Gibrat model are linked to long-range correlations \citep{rozenfeld2008laws} induced by spatial interactions. The influence of these amplifies the hierarchical differentiation between sizes of cities participating to exchanges in an urban system \citep{favaro2011gibrat}. The Simpop models include this effect by introducing, in an exogenous way to the model and at different times in the simulation, new urban functions which select some cities or are integrated by these in a continuous adaptation process to these innovations. In comparison to other Simpop models, SimpopLocal introduces two novel elements: it uses an abstract representation of successive innovation waves and gathers all in a single object ``innovation''. A second originality consists in making the innovation creation process endogenous by linking it to the size of the inhabited place, which is assumed to amplify in a non-linear way the emergence of new technical, social or cultural forms (with a creation probability varying as the square of populations being present or in relation). This more parsimonious of the model construction allow to significantly redue the number of parameters and authorizes thus more systematic exploration and evaluation.



%3.3 Implémentation de SimpopLocal, de Netlogo à OpenMole
\subsection{Implementation of SimpopLocal, from NetLogo to OpenMOLE}


%Simpoplocal a été initialement développé avec le langage Netlogo, puis re-développé avec le langage de programmation Scala. La simulation avec Netlogo a bénéficié des facilités de l’interface qui permet de suivre numériquement et graphiquement les modifications engendrées sur les variables macroscopiques qui résument l’état du système, mais a montré très vite ses limites en termes d’expérimentation. La méthode manuelle de réglage des valeurs des paramètres permettait difficilement d’éviter les “emballements” de la croissance urbaine conduisant à des accroissements de taille des villes bien trop énormes pour l’époque historique qu’il était question de simuler. La reprogrammation en Scala puis le passage sur la plateforme OpenMole devaient permettre une exploration plus précise et complète des comportements du modèle. 
SimpopLocal has been initially developed under the NetLogo language, and then redeveloped with the Scala programming language. The simulation with NetLogo has the advantage of the ergonomy of the interface which allows to numerically and graphically follow the modifications impacted on macroscopic variables describing the state of the system, but rapidly showed its limits in terms of possible experiments. The manual method of trial-and-error to set parameter values was difficultly capable of avoiding ``cascades'' of urban growth leading to city size increments too much larger for the historical period that it tried to simulate. The reprogramming in Scale and the integration into the OpenMOLE platform were to allow a more refined and complete exploration of model behaviors.


%Le modèle représente l'évolution des unités de peuplement dispersées dans une zone suffisamment grande pour accueillir quelques milliers d'habitants mais suffisamment limitée en surface pour assurer la connexion possible entre les lieux habités en fonction des moyens de transport disponibles à l'époque (par exemple il pourrait s'agir de l’ancienne Mésopotamie ou de la méso-Amérique antique). L'espace de simulation est composé d’une centaine de lieux habités. Chaque lieu est considéré comme un agent fixe et est décrit par trois attributs: l'emplacement de son habitat permanent (x, y), la taille de sa population P et les ressources disponibles dans son environnement local. La quantité de ressources disponibles R est quantifiée en unités d’habitants et peut être comprise comme la capacité de charge de l’environnement local pour soutenir une population, laquelle varie en fonction des compétences en exploitation des ressources que la population locale a acquises, grâce aux innovations qu’elle a créées ou reçues des autres lieux habités. Toutefois, l’exploitation des ressources est effectuée localement et le partage ou l'échange de biens ou de personnes ne sont pas explicitement représentés dans le modèle. Chaque nouvelle innovation créée ou acquise par un lieu habité développe ses compétences en exploitation. L’entité innovation s’entend ici comme une grande invention abstraite socialement acceptée, qui pourrait représenter une invention technique, une découverte, une organisation sociale, de nouvelles habitudes ou pratiques ... Chaque acquisition d’innovation par un lieu habité apporte la possibilité de surpasser ses seuils de capacité, et par conséquent autorise une croissance démographique.
The model represents the evolution of settlement units dispersed in a zone large enough to count a few thousand of inhabitants but limited enough in surface to ensure the possible connection between the inhabited places with the transportation means available at this time (this could be for example ancient Mesopotamy or antic meso-America). The simulation space is composed of around hundred inhabited places. Each place is considered as a fixed agent and is described by three attributes: the location of its permanent habitat $(x,y)$, the size of its population $P$ and the resources available in its local environment. The quantity of available resources $R$ is quantified in population units and can be understood as the carrying capacity of the local environment to sustain a population, which varies as a function of competencies to exploit resources that the local population acquired, thanks to the innovations it created or received from other inhabited places. However, the exploitation of resources is done locally and the share or the exchange of goods or of people are not explicitly included in the model. Each novel innovation created or acquired by an inhabited place develops its competencies in exploitation. The entity innovation corresponds here as a significant abstract invention socially accepted, which can be a technical innovation, a discovery, a social organisation, new habits or practices, \ldots Each acquisition of an innovation by an inhabited place brings the possibility to overpass its carrying capacities, and therein yields a demographic growth.


%Le modèle a été conçu pour être le plus parcimonieux possible, en minimisant le nombre des attributs des agents (qui sont des lieux habités) et les paramètres qui contrôlent leur évolution. On a utilisé directement les ordres de grandeur moyens indiqués par les travaux des archéologues pour fixer à environ 4000 ans la durée de la période de transition entre un système de peuplement agraire et un système de peuplement urbain, pour estimer un taux de variation moyen annuel de la population d’environ 0,02\% par an et pour considérer que la taille du plus grand lieu habité du système allait passer d’une centaine à quelque 10 000 habitants. En revanche, les valeurs de cinq autres paramètres ne pouvaient pas être estimées d’après la littérature et devaient être déduites des expériences de simulation. Il s’agit de la probabilité de création d’une innovation par interaction entre deux personnes d’un même lieu, de la probabilité de diffusion d’une innovation par interaction entre deux personnes de lieux différents, de l’intensité de l’effet dissuasif de la distance sur ces interactions entre lieux, et de l’impact d’une innovation sur la croissance de la population (qui passe par un accroissement des ressources disponibles) et de la dimension maximale possible d’un lieu habité (mesurée en termes de population ou de ressources disponibles) qui intervient dans l’équation de croissance logistique retenue comme modèle générique d’une évolution encore très fortement contrainte par les ressources locales. Les équations qui résument le modèle et les tableaux définissant précisément les paramètres et leur action sont détaillés dans (Schmitt 2014) et (Schmitt, Rey Coyrehourcq in Pumain, Reuillon, 2017, pp 21-34).
The model was conceived to be the most parcimonious possible by minimizing the number of attributes of agents (which are inhabited places) and the parameters controlling their evolution. The average orders of magnitude given by the work of archeologists were directly used to fix at around 4000 years the duration of the transition period between an agricultural settlement system and an urban settlement system, to estimate an annual average variation rate of population at around 0.02\% per year, and to consider that the size of the largest inhabited place in the system would change from around hundred to around 10,000 inhabitants. On the contrary, the values of the five other parameters could not be estimated from the literature and had to be deduced from simulation experiments. These parameters are the probability to create an innovation through an interaction between two persons in the same place, of the diffusion probability of an innovation through an interaction between two persons from two different places, of the intensity of the dissuasive effect of distance on these interactions between places, of the impact of an innovation on population growth (which is a consequence of an increase of available resources) and of the maximal possible dimension of an inhabited place (measured in terms of population or available resources) which intervenes in the logistic growth equation retained as a generic model of an evolution still very strongly constrained by local resources. Equations summarizing the model and tables defining precisely parameters and their action are detailed in \cite{schmitt2014modelisation} and \cite{pumain2017urban}.


%On définit une valeur initiale pour la population et les ressources des lieux habités, puis le réseau d’interaction entre eux est créé. Ensuite, à chaque étape de la simulation, les mécanismes de croissance de la population et de diffusion de l'innovation sont appliqués. L’impact des innovations sur l’efficacité de l’extraction des ressources est calculé. Cette boucle est itérée jusqu'à ce que le critère d'arrêt soit atteint: dans ce cas, après 4000 étapes ou lorsqu’un nombre maximal arbitraire d'innovations a été atteint. On observe l’évolution de l’état du système de peuplement défini au niveau macro-géographique par la distribution de la taille des lieux habités, résumée par la pente de la distribution rang-taille. Le modèle utilisant certains paramètres qui sont des probabilités est stochastique, un même jeu de valeurs de paramètres peut donner lieu à des résultats sensiblement différents. Une méthode automatisée pour faire varier les valeurs des paramètres et interpréter les résultats obtenus a été mise au point progressivement par une collaboration entre informaticiens et géographes.
We define an initial value of population and resources of inhabited places, and the interaction network between them is created. Then at each stage of the simulation, mechanisms for population growth and innovation diffusion are applied. The impact of innovations on the efficacy of resource extraction is computed. This loop is iterated until the stopping criteria is reached: in this case after 4000 steps or when an arbitrary maximal number of innovations is reached. We observe the evolution of the settlement system state defined at the macro-geographic level through the distribution of the size of inhabited places, summarized by the slope of the rank-size distribution. The model uses some parameters which are probabilities and is stochastic, therefore a same set of parameter values can yield quite different results. An automatized method to make the value of parameters vary and interpret the results obtained has been progressively elaborated through a collaboration between computer scientists and geographers.



%3.4 Calibrage et validation
\subsection{Calibration and validation}



%L’automatisation de l’exploration des dynamiques engendrées par les modèles de simulation avec la plateforme OpenMole utilise des algorithmes génétiques qui réalisent de façon systématique les variations des valeurs des paramètres auparavant effectuées “à la main” par le chercheur. La distribution des calculs sur une infrastructure de grille (un réseau d’ordinateurs) permet en outre de conduire ce très grand nombre d’opérations combinatoires en réduisant considérablement le temps de calcul, grâce au traitement en parallèle de l’information. Mais la mise en oeuvre de cette nouvelle forme de l’expérimentation des modèles suppose aussi une intervention du chercheur thématicien, qui doit sélectionner les objectifs précis que son modèle doit satisfaire, tandis qu’un raffinement supplémentaire de la méthode d’exploration peut conduire à un renforcement de la confiance qu’il accorde aux hypothèses scientifiques de son modèle.
The automation of the exploration of dynamics produced by simulation models with the OpenMOLE platform uses genetic algorithms which realize in a systematic way the variations of parameter values which were before done ``by hand'' by the researcher. The distribution of computation tasks on a grid infrastructure (a network of computers) furthermore allows to operate this very high number of combinatorial operations while significantly reducing the computing time, through the parallel processing of information. But putting this new form of model experiments into practice also assumes an intervention of the thematic researcher, which has to select the precise objectives that the model has to satisfy, whereas a supplementary refinement of the exploration method can lead to an increase in trust given to the scientific assumptions of the model.


%Le calibrage comme optimisation au moyen des algorithmes génétiques
\subsubsection*{Calibration as optimization through genetic algorithms}

%Le calibrage est une procédure qui cherche à minimiser l’écart (appelé fitness) entre le comportement simulé par le modèle et le comportement observé empiriquement, en faisant varier de façon incrémentale les valeurs inconnues des paramètres du modèle. Stonedahl (2011) a rappelé les difficultés de cette exploration qui devient vite fastidieuse lorsqu’elle est conduite manuellement, à cause des multiples bifurcations intervenant dans des modèles où la plupart des mécanismes liant les variables sont non linéaires. Une exploration exhaustive de l’espace des paramètres n’est pas envisageable car elle exigerait des temps de calcul trop importants, en croissance exponentielle avec le nombre de ces paramètres. Comme ces procédures produisent aussi de grandes quantités de résultats, elles exigent en outre d’employer des méthodes adaptées pour traiter et visualiser les informations engendrées par les simulations. Tout un ensemble de logiciels doit donc être mis au point pour permettre au chercheur de découvrir les principaux schémas des dynamiques associées aux variations des paramètres de son modèle.
Calibration is a procedure aiming at minimizing the discrepancy (called fitness) between the behavior simulated by the model and the behavior observed empirically, by making the unknown model parameter values vary in an incremental way. \cite{stonedahl2011genetic} has recalled the difficulties of this exploration which rapidly becomes fastidious when it is done by hand, because of the multiple bifurcations occurring in models in which most of mechanisms linking variables are non-linear. An exhaustive exploration of the parameter space can not be conceived since it would require a too large computation time, which follows an exponential growth in the number of these parameters. As these procedures also produce large quantities of results, they furthermore impose to use dedicated methods to process and visualize informations produced by simulations. A whole suite of softwares must thus be elaborated to enable the researcher to discover the main schemes for dynamics associated to the variations of model parameters.


%C’est là où des procédures informatiques adaptées peuvent être utilisées, en rapportant la question du calibrage à un problème d’optimisation. Les algorithmes génétiques ont été utilisés pour calibrer des systèmes multi-agents dans plusieurs domaines, en médecine (Castiglione et al, 2007), en écologie (Duboz et al, 2010), en économie (Espinosa, 2012; Stonedahl et Wilensky, 2010a), ou en hydrologie (Solomatine et al, 1999). En dépit de la large utilisation des systèmes multi-agents en sciences sociales, cette méthode n’a pas été appliquée très souvent (Heppenstall et al, 2007; Stonedahl et Wilensky, 2010b). Ce type d’expérience numérique exige en effet que soient définis des objectifs quantitatifs permettant d’évaluer si les résultats de la simulation sont compatibles avec les attentes des experts, il faut également savoir gérer l’énorme charge de calcul et parvenir à optimiser une fonction de fitness susceptibles de très importantes variations stochastique (Pietro et al, 2004). 
%  reformulate on stichasticity ? no need, general
This is where suited algorithms can be used, by understanding the calibration issue as an optimization problem. Genetic algorithms have been used to calibrate multi-agent systems in several domains as therapeutic evaluation \citep{castiglione2007optimization}, ecology \citep{duboz2010application}, economics \citep{espinosa2012genetic,stonedahl2010evolutionary}, or hydrology \citep{solomatine1999automatic}. Despite of the the frequent use of multi-agent systems in social sciences, this method has not been applied very often \citep{heppenstall2007genetic,stonedahl2010evolutionary}. This type of numerical experiment indeed requires that quantitative objective are defined in order to evaluate if simulation results are compatible to results expected from expert knowledge, and the enormous computation load must also be managed while optimizing a fitness function candidate to very large stochastic fluctuations \citep{di2004applying}.


%Dans le cas de SimpopLocal, qui comprend 5 paramètres dont les valeurs sont inconnues (même leurs ordres de grandeur ne peuvent pas être estimés à partir de données empiriques), nous avons dû identifier trois “fonctions objectif”. Celles-ci caractérisent un résultat de simulation au niveau macro-géographique et correspondent à des faits stylisés dont les ordres de grandeur ont pu être établis à partir des connaissances archéologiques et historiques: la distribution finale des tailles de villes doit être lognormale (peu différente d’une loi de Zipf), la taille maximale qu’atteint la plus grande ville doit être d’environ 10 000 habitants, pour une durée de simulation équivalant à 4000 ans.
In the case of SimpopLocal, which has five unknown parameter values (even their order of magnitude can not be estimated from empirical data), three ``objective functions'' had to be defined. These characterize a simulation result at the macro-geographical level and correspond to stylized facts which order of magnitude were established from archeological and historical knowledge: the final distribution of city sizes must be log-normal (not far to a Zipf's law), the maximal size of the largest city must be around 10,000 inhabitants, for a simulation duration corresponding to 4000 years.



%Cette obligation de définir des fonctions-objectif  pourrait être considérée comme une contrainte forte sur la validité épistémologique du modèle, elle semble en effet contredire l’hypothèse d’une évolution ouverte pour les systèmes de villes. En fait, cette étape intermédiaire de calcul représente un comprimé de connaissances, notre exigence a minima sur la représentativité et la plausibilité du comportement du modèle par rapport à l’ensemble envisageable des dynamiques des villes en système (à l’époque historique de l’émergence des villes). Le résultat en termes d’évaluation des simulations doit permettre d’avancer dans la connaissance des processus d’interaction intra-urbains susceptibles d’engendrer dette dynamique générale à l’échelon macroscopique du système, cette reconstitution théorique s’apparentant alors à ce que des physiciens nomment le “problème inverse”.
This requirement of defining objective functions could be considered as a strong constraint on the epistemological validity of the model, it seems indeed to contradict the assumption of an open evolution for systems of cities. Indeed, this intermediate computation step corresponds to a condensate of knowledges, a minimal requirement on the representativity and the plausibility of model behavior in comparison to all dynamics of systems of cities that could be considered (at the historical period of the emergence of cities). The result in terms of evaluation of simulations should allow to advance in the knowledge of intra and inter-urban interaction processes susceptible to produce this general dynamic at the macroscopic scale of the system, such a theoretical reconstitution corresponding then to what some physicists describe as an ``inverse problem''.



%Un domaine de variation numérique assez large est établi a priori pour chacun des cinq paramètres. Chaque jeu de paramètres, combinant une valeur pour chacun d’entre eux, est évalué en fonction de la sortie de simulation qu'il produit. Cette évaluation mesure la proximité entre les sorties de la simulation et les fonctions objectifs définies pour le modèle et permet ainsi de mesurer la capacité d’un certain ensemble de valeurs de paramètres à reproduire les faits stylisés que la simulation doit approcher au mieux. Les paramétrages recevant les meilleures évaluations sont ensuite utilisés comme base pour engendrer de nouveaux jeux de paramètres qui sont ensuite testés. 
A rather broad numerical domain is established a priori for each of the five parameters. Each parameter set combining a value for each parameter is evaluated depending on the simulation outcome it produces. This evaluation measures the proximity between simulation outputs and objective functions defined for the model and provides thus a measure of the ability of a given of parameters values set to reproduce the stylized facts of which the simulation must be the closest. The parameter values receiving the best evaluations are then used as a basis to produce new parameter sets which are then tested.


%Exploration de l’espace des paramètres sous contrainte d’objectifs
\subsubsection*{Exploration of the parameter space under objective constraints}


%Le modèle SimpopLocal étant stochastique, les résultats de la simulation varient d'une simulation à l'autre pour le même paramétrage. Par conséquent, l'évaluation du paramétrage en fonction des trois objectifs doit prendre en compte cette variabilité. Nous avons vérifié qu’une centaine de simulations pour chaque jeu de paramètres suffisait à saisir cette variabilité sans trop augmenter la durée du calcul. 
The SimpopLocal model being stochastic, simulation results vary from one simulation to the other for the same parameter values. In consequence, the evaluation of parameter values according to the three objectives must take into account this variability. We verified that around hundred simulations for each parameter set was enough to capture this variability without increasing too much computation time.


%A chaque fonction-objectif correspond une mesure de l’évaluation de la qualité du résultat simulé.  La capacité du modèle à produire une distribution log-normale est mesurée par l’écart entre la distribution simulée et une distribution log-normale théorique ayant même moyenne et écart type selon un test de Kolmogorov-Smirnov.  L’objectif de population maximale quantifie la capacité du modèle à engendrer des villes plus ou moins grandes, le résultat d'une simulation est testé en calculant l'écart entre la taille de la plus grande agglomération et la valeur attendue de 10 000 habitants: [(population de la plus grande agglomération −10 000) / 10 000 |. L’objectif de la durée de la simulation quantifie la capacité du modèle à générer une configuration attendue dans un laps de temps historiquement plausible. On calcule l’écart entre le nombre d’itérations de la simulation et la valeur attendue de 4000 étapes de la simulation: | (simulation durée −4000) / 4000 |. Ces trois calculs d’erreur sont normalisés afin de pouvoir comparer le degré de réussite d’une simulation vis-à-vis de chacun des trois objectifs. Mais l’agrégation des trois calculs qui produirait une seule mesure de qualité globale n’étant pas possible, un algorithme multi-objectif est nécessaire pour déterminer quelles simulations sont les plus satisfaisantes pour approcher la configuration finale souhaitée. Ce type d’algorithme calcule des solutions de compromis telles qu’aucune ne domine toutes les autres pour tous les objectifs. Ces solutions sont appelées des compromis de Pareto et elles forment ensemble ce qui est appelé un front de Pareto.
% rq : why normalize then as the nsga2 is ranked-based ? => crowding distance !
To each objective function corresponds a measure of the evaluation to the quality of the simulated result. The ability of the model to produce a log-normal distribution is measured by the distance between the simulated distribution and a theoretical log-normal distribution with the same average and standard deviation following a Kolmogorov-Smirnov test. The maximal population objective quantifies the ability of the model to produce more or less large cities, the result of a simulation being tested by computing the distance between the size of the largest city and the expected value of 10,000 inhabitants: $\left|(\textrm{population of the largest agglomeration} - 10,000) / 10,000 \right|$. The objective of the duration of the simulation duration quantifies the ability of the model to generate an expected configuration in an historically plausible time window. The distance between the number of iterations of the simulation and the expected value of 4000 steps for the simulation is computed: $\left| (simulation duration - 4000) / 4000 \right|$. These three computations of errors are normalized in order to enable the comparison the degree of satisfaction of a simulation regarding each of the three objectives. But the aggregation of the three computations which would produce a single global quality measure is not possible , a multi-objective algorithm is necessary to determine which simulations are the most satisfying to approach the expected final configuration. This type of algorithm computes compromise solutions such that no solution dominates all the others for all objectives. These solutions are called Pareto compromises and are together part of what is called a Pareto front.


%L’utilisation de méthodes d’exploration globales comme celle des algorithmes génétiques pour calibrer un modèle multi-agent (et en particulier un modèle multi-agent stochastique) implique un coût de calcul très élevé (Sharma et al, 2006). Ce type de charge est trop volumineux pour être exécuté sur des ordinateurs locaux, et les supercalculateurs sont très coûteux et ne sont pas facilement disponibles dans la plupart des laboratoires. Les grilles informatiques offrent une solution pour résoudre ces problèmes de calculs intensifs. Cependant, l’informatique à une si grande échelle suppose d’orchestrer l'exécution de dizaines de milliers d'instances du modèle sur des ordinateurs distribués dans le monde entier. La probabilité cumulée de pannes locales et le problème de répartir la charge de travail de façon optimale sur la grille rendent très difficile son utilisation pour un chercheur non spécialisé, comme précisé ci-dessus. C’est entre autres pour surmonter ces difficultés que la plate-forme OpenMOLE a été construite (Reuillon et al, 2010; 2013). Cet exemple de la calibration du modèle SimpopLocal montre bien dans quelle mesure OpenMOLE aide les modélisateurs à franchir le fossé technique et méthodologique qui les sépare de l'informatique haute performance.
The use of global exploration methods such as the one of genetic algorithms to calibrate a multi-agent model (and in particular a stochastic multi-agent model) implies a very high computation cost \citep{sharma2006multi}. Such a computation charge is too voluminous to be executed on local computers, and supercomputers have a high cost and are not easily available for most of research laboratories. Computer grids provide a solution to these issues of intensive computation. However, distributing computation at such a large scale requires to orchestrate the execution of ten of thousands of model instances on computers distributed in the entire world. The cumulated probability of local failures and the issue of sharing to workload in an optimal way on the grid make its use very difficult for a non-specialized researcher as described above. It is in particular to overcome these difficulties that the OpenMOLE platform has been conceived \citep{reuillon2010declarative,reuillon2013openmole}. This exemple of the calibration of the SimpopLocal model shows well to what extent OpenMOLE helps modelers to overcome the technical and methodological gap which separates them from high performance computing.


%L’infrastructure de la grille de calcul (EGI) nous a permis d’utiliser une puissance de calcul telle qu’un demi milliard d’exécutions du modèle ont pu être effectuées pour le calibrage de SimpopLocal, lequel sans cela aurait requis quelque 20 années de calcul avec un seul ordinateur.
The infrastructure of the European Computation Grid allowed to use a computing power such that half a billion simulations of the model have been executed for the calibration of SimpopLocal, without which it would have required around 20 years of computation with a single computer.



%Le profil de calibrage, un grand saut épistémologique pour les SHS
\subsubsection*{The Calibration Profile, a significant leap forward for social sciences}


%Le résultat du processus de calibrage assure seulement que le modèle peut reproduire les caractéristiques stylisées de l’émergence d’un système de villes, avec une évaluation assez précise des valeurs des paramètres qui toutes ensemble contribuent à assurer cette évolution. Mais il ne dit rien de la fréquence à laquelle les jeux de paramètres produisent des comportements plausibles, et de quelle façon chaque paramètre contribue à modifier le comportement du modèle. Il serait intéressant par exemple de savoir à quel moment certaines valeurs de paramètre empêchent le système d’atteindre un comportement plausible, et de ne pas se restreindre à ne connaître qu'un seul jeu de valeurs de paramètres «optimales». 
The result of the calibration process ensures only that the model is able to reproduce stylized characteristics of the emergence of a system of cities, with a relatively precise evaluation of parameter values which altogether contribute to produce this evolution. But it does not tell anything on the frequency to which parameter sets produce plausible behaviors, and to what extent each parameter contributes to modify the model behavior. It would be interesting for example to know at which point some parameter values avoid the system to reach a plausible behavior, and to not be restricted to know only a single ``optimal'' parameter values set.


%Une nouvelle méthode a été mise au point pour représenter la sensibilité du modèle aux variations d'un seul paramètre, indépendamment des variations de tous les autres paramètres (Reuillon et al. 2015). Au moyen d’une fonction qui calcule une seule valeur numérique décrivant la qualité du calibrage pour le modèle, l’algorithme de profil calcule l’erreur de calibrage la plus faible possible lorsque la valeur d'un paramètre donné est fixée et que les autres sont libres. L’algorithme calcule cette erreur minimale pour tout le domaine de variation du paramètre étudié. Pour chaque valeur d’un paramètre, l’algorithme cherche à identifier les jeux de valeurs des autres paramètres qui produisent le meilleur ajustement du modèle aux données attendues (la plus petite erreur possible). Un graphique représente alors les variations de cette valeur d’ajustement optimale en fonction des variations du paramètre étudié. Le profil de calibrage montre ainsi plusieurs formes possibles pour cette courbe. Lorsqu’elle présente une nette inflexion vers les valeurs les plus basses pour l’erreur de calibrage, cela pour un tout petit domaine de variation des valeurs du paramètre étudié, on peut en conclure qu’on a vraiment identifié l’ordre de grandeur du paramètre qui satisfait aux exigences en termes de comportement du modèle. Si l’une de ces courbes reste plate, cela indique que le paramètre n’a pas d’effet sur le comportement du modèle et peut donc en être éliminé. Ainsi, dans le cas de SimpopLocal, un paramètre imaginé comme le durée de vie d’une innovation a été finalement exclu car des variations restaient sans effet sur la qualité d’ajustement du modèle, toutes choses égales quant aux variations des autres paramètres (Schmitt, 2014). On a donc ici la possibilité d’évaluer jusqu’à quel point les mécanismes imaginés pour construire le modèle sont non seulement suffisants, mais aussi nécessaires pour produire le comportement attendu. Certes dans les limites du cadre théorique et de la sélection des faits stylisés retenus, c’est la première fois que des chercheurs en SHS peuvent parvenir à ce type de conclusion scientifique essentielle, grâce à une méthode de validation enfin efficace pour les modèles de simulation multi-agents. C’est un immense progrès du point de vue épistémologique en sciences sociales – certes toujours dans le cadre théorique donné par les objets, attributs et mécanismes sélectionnés par les chercheurs pour être représentatifs du système observé.
% wrong use of inflexion (// eco ?) -> current lang
A new method has been elaborated to represent the sensitivity of the model to the variations of a single parameter, independently of the variations of all other parameters \citep{reuillon2015new}. Using an objective function computing a single numerical value describing the quality of the calibration for the model, the profile algorithm computes the calibration error the lowest possible when the value of a given parameter remains fixed and all others are free. The algorithm computes this minimal error for the full variation domain of the parameter studied. For each value of a parameter, the algorithm aims at identifying parameter values for other parameters which produce the best fit of the model to the expected data (the smallest error possible). A plot gives then the variations of this optimal fit value as a function of the variations of the parameter studied. The calibration profile shows then different possible forms for this curve. When it exhibits a clear inflexion in the lowest values for the calibration error, for a relatively small domain of variation for the parameter studied, it is possible to conclude that the order of magnitude of the parameter was truly identified which satisfies to the expectations in terms of model behavior. If one of these curves remains flat, it indicates that the parameter has no effect on the local behavior of the model and thus can be removed from it. Therefore, in the case of SimpopLocal, a parameter thought as the duration of life of an innovation was finally excluded since variations remained without effect on the quality of fit of the model, all things being equal regarding the variations of the other parameters \citep{schmitt2014modelisation}. This provides thus the possibility to evaluate to what extent the mechanisms invented to construct the model are not only sufficient, but also necessary to produce the expected behavior. Naturally within the limits of the theoretical framework and of the selection of stylized facts which were kept, it is the first time that researchers in social science can reach such an essential scientific conclusion, thanks to a validation method finally efficient for multi-agent simulation models. This is an enormous progress from the epistemological point of view for social sciences - surely still within the theoretical framework given by the objects, attributes and mechanisms selected by researchers as representatives of the system observed.


%Une forme complémentaire de validation du modèle pourrait être alors imaginée si des historiens archéologues tentaient de le recalibrer avec des données de leurs observations. En effet, le jeu de paramètres estimé contient des valeurs qui engendrent bien la dynamique voulue pour un système de peuplement mais qui ne sont pas fixées dans l’absolu, elles sont  relatives les unes aux autres d’une part et aux données fictives introduites d’autre part. Si l’on modifie ces dernières pour les rendre compatibles avec un système de peuplement historiquement observé, la capacité du modèle à simuler son développement serait alors confirmée, non seulement en reconstruisant les trajectoires de l’évolution de la population des lieux habités considérés, mais aussi en conservant les ordres de grandeur relatifs des paramètres qui engendrent cette dynamique. 
A complementary form of model validation could then be imagined if historians and archeologists would try to recalibrate it from their observations. Indeed, the estimated parameter set contains values which indeed provide the expected dynamic for a settlement system but which are not absolutely fixed, they are relative one to the other on the one hand and to the synthetic data introduced on the other hand. If these are modified to be compatible with an historically observed settlement system, the ability of the model to simulate its development would then be confirmed, not only by reconstructing the trajectories of the evolution of population of the considered inhabited places, but also by keeping the relative orders of magnitude of parameters which yield this dynamic. 




%4 Exemples d’applications d’OpenMOLE : modèles d’interaction réseaux-territoires
\section{Examples for applications of OpenMOLE: network-territories interaction models}


%Nous proposons dans cette section d’illustrer l’application des méthodes d’exploration d’OpenMOLE et du calcul intensif à une autre question thématique, celle des interactions entre réseaux et territoires. Cette question a alimenté de nombreux débats scientifiques, pour lesquels la plupart des questions restent relativement ouvertes. Par exemple, le problème des “effets structurants des infrastructures de transport” (Bonnafous et Plassard, 1974), présenté par (Offner, 1993) comme un “mythe scientifique” invoqué pour justifier le coût d’une nouvelle infrastructure par ses retombées sur le développement régional, pas toujours observées à moyen terme, peut selon A. Bretagnolle dans (Offner et al., 2014) être observé pour des territoires plus vastes et sur le temps long, tout en tenant compte des fluctuations locales dans les dynamiques des systèmes de villes. La difficulté empirique d’extraire des faits stylisés généraux ainsi que la difficulté conceptuelle d'entités géographiques en relations de causalités circulaires, sont contournées par l’approche de modélisation de la co-évolution des réseaux de transport et des territoires proposée par Raimbault (2018b). Les résultats obtenus sont étroitement liés à l’utilisation d’OpenMOLE et de ses algorithmes d’exploration et de calibrage, dont nous allons donner quelques illustrations.
%  wrong cit thesis ? ok pas besoin paper synth.
We propose in this section to illustrate the application of the exploration methods included in OpenMOLE and intensive computation to an other thematic question, the issue of interactions between networks and territories. This question has fed numerous scientific debates fir which most of issues remain relatively open. For example, the issue of ``structuring effects of transportation infrastructures'' \citep{bonnafous1974methodologies}, described by \cite{offner1993effets} as a ``scientific myth'' invoked to justify the cost of an infrastructure through its spillovers on regional development, and which are not always observed on middle terms, can according to A. Bretagnolle in \citep{offner2014effets} be observed for broader territories and on long times, while taking into account local fluctuations in dynamics of systems of cities. The empirical difficulty to extract general stylized facts together with the conceptual difficulty of geographical entities in relations of circular causality, are avoided through the approach of modeling co-evolution between transportation networks and territories proposed by \cite{raimbault2018modelisation}. The results obtained are closely linked to the use of OpenMOLE and its exploration and calibration algorithms, of which we will give a few illustrations.



%L’application de calibrage multi-objectif s'avère essentielle pour l’application des modèles de systèmes de villes à des situations réelles. Par exemple, (Raimbault, 2018a) introduit un modèle d'évolution d’un système de villes sur le temps long, proche du modèle de (Favaro et Pumain, 2011), mais se concentrant sur l’effet du réseau de transport physique. Les taux de croissance des villes sont déterminés par la superposition de plusieurs effets: (i) croissance endogène capturée par un taux de croissance fixe correspondant au modèle de Gibrat; (ii) interactions entre villes par un modèle gravitaire; (iii) rétroaction des flux circulant dans le réseau sur les villes traversées. Ce modèle est calibré de manière non stationnaire dans le temps (c’est-à-dire sur des fenêtres temporelles glissantes, afin de prendre en compte le changement de nature des dynamiques urbaines, comme observé par Bretagnolle et Franc (2018) avec par exemple les mutations des réseaux de transport), sur le système de villes français entre 1830 et 2000. Pour calibrer le modèle, les populations simulées sont comparées aux populations observées. À ce stade, l’utilisation d’un algorithme de calibrage multi-objectif (l’algorithme NSGA2 implémenté dans OpenMOLE) est essentielle. En effet, l’ajustement peut par exemple s’effectuer sur une erreur carrée moyenne dans le temps et pour l’ensemble des villes. Cependant, vu les disparités de taille des villes liées à la hiérarchie urbaine, il émerge rapidement qu’une optimisation mono-objectif sur cette erreur s'attellera à ajuster la taille des plus grandes villes, au détriment de la majorité des villes du système. L’ajout d’un second objectif, pris par exemple comme une erreur carrée moyenne sur les logarithmes des populations, permet de prendre celles-ci en compte. Un résultat important de (Raimbault, 2018a) est alors l'émergence de fronts de Pareto pour ces deux objectifs, pour l’ensemble des fenêtres temporelles considérées. Cela montre que ce type de modèle doit être appliqué en faisant un compromis entre l’ajustement des populations pour les villes moyennes et des populations pour les plus grandes villes. Ce résultat est permis grâce à l’optimisation multi-objectif par algorithme génétique d’OpenMOLE.
The application of multi-objective calibration appears to be essential for the application of models for systems of cities to real situations. For example, \cite{raimbault2018indirect} introduce a model for the evolution of a system of cities on long times which is close to the model by \citep{favaro2011gibrat} but focuses on the effect of the physical transportation network. Growth rates of cities are determined by the superposition of several effects: (i) endogenous growth captured with a fixed growth rate corresponding to the Gibrat model; (ii) interactions between cities through a gravity model; (iii) feedback of flows circulating in the network on traversed cities. This model is calibrated in a non-stationary way in time (i.e. on temporal moving windows in order to take into account the change in nature of urban dynamics such as observed by \cite{bretagnolle2018vers} with for example the mutations of transportation networks) on the French system of cities between 1830 and 2000. To calibrate the model, simulated populations are compared to observed populations. At this stage the use of a multi-objective calibration algorithm (the NSGA2 algorithm implemented in OpenMOLE) is crucial. Indeed, the fit can be for example computed as a mean square error in time and for all cities. However, given the disparities in city sizes due to urban hierarchy, it rapidly occurs that a mono-objective calibration on this error will focus on adjusting the size of largest cities, at the expense of most cities in the system. The addition of a second objective taken for example as a mean square error on logarithms of populations, allowing to take these into account. An important result of \citep{raimbault2018indirect} is then the emergence of Pareto fronts for these two objectives for all time windows considered. This shows that this type of model must be applied by making a compromise between the adjustment of population for medium-sized cities and populations for largest cities. This result is obtained thanks to the multi-objective optimization with a genetic algorithm in OpenMOLE.



%Un autre exemple d’application des méthodes de la plateforme qui illustre son rôle crucial est donné par la recherche de régimes de co-évolution. Suivant Raimbault (2017b), l'étude des motifs de corrélation retardée dans le temps permet d’isoler des régimes typiques d’interaction entre variables de réseau et variables de territoires. Plus précisément, Raimbault (2018b) définit la co-évolution comme l’existence de relations circulaires causales, au niveau d’un ensemble d’entités dans une certaine emprise spatiale. Dans le cas des réseaux et des territoires, les propriétés des réseaux doivent être localement causées par celles des territoires, et réciproquement. Des causalités unidirectionnelles des réseaux vers les territoires correspondent alors aux “effets structurants” mentionnés ci-dessus. Cette définition permet de capturer la “congruence” (Offner, 1993) entre ces objets, en quelque sorte leur adaptation réciproque de manière dynamique. Elle permet aussi la construction d’une méthode opérationnelle proposée par Raimbault (2017b), qui cherche statistiquement des liens de causalité entre variables correspondantes. En pratique, la notion faible de causalité de Granger est mobilisée, permettant une flexibilité au regard des données nécessaires et du cadre temporel et spatial d’estimation. Cette causalité est dans notre cas quantifiée par les corrélation retardées entre variations des variables de réseau (comme les centralités ou l'accessibilité) et variations des variables de territoires (comme population, emplois, transactions immobilières, etc.), et l’existence de maxima significatifs à des retards non nuls donne une direction de causalité. Une typologie de ces profils de corrélations retardées fournit ce qu’on nomme des “régimes de causalité”, parmi lesquels des régimes de co-évolution où deux variables territoire et réseau sont en causalité réciproque.
An other application example for methods included in the platform which illustrate its crucial role is given by the search for co-evolution regimes. Following \cite{raimbault2017identification}, the study of lagged correlation patterns in time allows to identify typical interaction regimes between variables describing the network and variables describing the territory. More precisely, \cite{raimbault2018modelisation} defines co-evolution as the existence of circular causal relationships at the level of a population of entities in a given spatial extent. In the case of networks and territories, network properties must be locally caused by the properties of territories and reciprocally. Mono-directional causalities of networks to territories correspond then to ``structuring effects'' mentioned above. This definition allows to capture the ``congruence'' \cite{offner1993effets} between these objects, in some sense their reciprocal adaptation in a dynamical way. It also yields the construction of an operational method proposed by \cite{raimbault2017identification} which statistically investigates causality links between corresponding variables. In practice, the weak Granger causality notion is used, providing a flexibility regarding the data required and the temporal and spatial frame of estimation. This causality is in our case quantified by lagged correlations between variations of network variables (such as centralities or accessibilities) and variations of territory variables (such as population, employments, real estate transactions, etc.), and the existence of significant extrema at non zero lags gives a sense of causality. A typology of these lagged correlation profiles provides what we call ``causality regimes'', among which co-evolution regimes in which two variables for territory and network are in reciprocal causality.



%La question est alors dans un cas d'étude donné d’identifier les régimes présents à partir de données observées ou de données simulées par un modèle, et notamment ceux qui correspondent à une co-évolution. La démonstration de l’existence de tels régimes en sortie d’un “modèle de co-évolution” n’est pas a priori attendue, puisque les processus inclus à l'échelle microscopique où les influences sont en effet réciproques n’impliquent pas une causalité réciproque à l'échelle macroscopique des indicateurs, puisque les modèles considérés sont complexes et témoignent d’une émergence. Cette méthode est appliquée à un modèle macroscopique de co-évolution par Raimbault (2019a), qui étend le modèle de Raimbault (2018a) par l’ajout de règles d'évolution des capacités des liens du réseau. Un échantillonnage direct, qui consiste en un tirage aléatoire d’un nombre fixe de points de paramètres (par exemple par échantillonnage Hypercube Latin maximisant la répartition des points), est une première expérience permise par OpenMOLE pour avoir un aperçu de la capacité du modèle à produire de la co-évolution. Celui-ci permet d’isoler un certain nombre de régimes pouvant être potentiellement produits par le modèle (33 régimes pour 729 régimes possibles pour les variables considérées, i.e.  4,5\%  (dans ce cas on considère comme variable de territoire les populations, et comme variables de réseau la centralité de proximité et l'accessibilité, ce qui correspond à six couples dirigés de variables, et donc $3^6=729$ configurations possibles, chaque couple pouvant présenter corrélation retardée positive, négative, ou inexistante). On trouve parmi ceux-ci 19 régimes de co-évolution, dont l’existence ne pouvait pas être intuitivement prédite. L’existence et la variété de ces régimes est un résultat important, montrant qu’il est possible de modéliser une co-évolution, au sens statistique précis donné précédemment.
The question is then is a given case study to identify the regimes in presence from observed data or from data simulated by a model, and particularly the regimes corresponding to a co-evolution. The demonstration of the existence of such regimes as output of a ``co-evolution model'' is a priori not expected, since processes included at the microscopic scale for which the influences are indeed reciprocal do not imply a reciprocal causality at the macroscopic scale of indicators, as the models considered are complex and exhibit emergence. This method is applied to a macroscopic model of co-evolution by \cite{raimbault2018modeling}, which extends the model of \citep{raimbault2018indirect} by adding rules for the evolution of network capacities. A direct sampling which consists in a random sampling of a fixed number of parameter points (for example through Latin Hypercube Sampling maximizing the discrepancy of points), is a first experiment allowed by OpenMOLE to have an overview of the capacity of the model to produce co-evolution. This experiment provides a certain number of regimes which can potentially be produced by the model, namely 33 regimes among 729 possible regimes for the variable considered, i.e. 4,5\% (in this case we consider as territory variable the populations, and as network variables the closeness centrality and the accessibility, what corresponds to six directed couples of variables, and thus $3^6=729$ possible configurations since each couple can exhibit a positive, negative or inexistent lagged correlation). We find among these 19 co-evolution regimes, which existence could not be intuitively predicted. The existence and the variety of these regimes is an important result, showing that it is possible to model a co-evolution, in the precise statistical sense given above.



%L’application de l’algorithme Pattern Space Exploration (Cherel, Reuillon and Cottineau, 2015) avec comme objectif la diversité des régimes produits permet alors de considérablement étendre cette conclusion, puisque celui-ci produit 260 régimes (35,7\%). Il s’agit d’un exemple typique où la forte non-linéarité des sorties considérées peut mener à des conclusions partielles voire biaisées et où l’utilisation d’une méthode spécifique est cruciale. Les résultats sont alors rendus plus robustes et étendus, grâce à l’application d’une méthode spécifique intégrée a la plateforme OpenMOLE.
The application of the Pattern Space Exploration algorithm \citep{10.1371/journal.pone.0138212} with objective the diversity of produced regimes allows then to considerably extend this conclusion, since it produces 260 regimes (35,7\%). This is a typical example where the strong non-linearity of outputs considered can lead to partial or even biased conclusions and where the use of a specific method is crucial. The results are then made more robust and extended thanks to the application of a specific method integrated to the OpenMOLE platform.



%Cette méthode permet par ailleurs de comparer entre eux des modèles avec une certaines confiance dans l'exhaustivité des solutions obtenues. Raimbault (2019b) applique la même démarche au modèle SimpopNet introduit par Schmitt (2014), qui est également un modèle de co-évolution à l'échelle macroscopique et présentant un grand nombre de points communs avec le modèle précédent notamment dans les variables considérées et donc les indicateurs de sortie calculables. Il est alors obtenu un nombre plus faible de régimes d’interaction et de régimes de co-évolution, confirmant d’une part qu’il n’est pas immédiat pour un modèle conçu pour la co-évolution de faire effectivement émerger des régimes de co-évolution, et suggérant par ailleurs que des contraintes plus fortes dans les règles d'évolution du réseau induisent une plus grande difficulté à produire une diversité de régimes. 
This method furthermore allows to compare between them models with a certain confidence in the exhaustivity of solutions obtained. \cite{raimbault2018unveiling} applies the same approach to the SimpopNet model introduced by \cite{schmitt2014modelisation} which is also a co-evolution model at the macroscopic scale exhibiting a large number of common points with the previous model in particular in the variables considered and thus in the output indicators that can be computed. A smaller number of regimes of interaction and of co-evolution is then obtained, confirming on the one hand that it is not straightforward for a model conceived for co-evolution to effectively produce co-evolution regimes, and on the other hand suggesting that stronger constraints in the evolution rules for the network induce a bigger difficulty to produce a diversity of regimes.



%5 Perspectives
\section{Perspectives}

%L'élaboration de la plateforme OpenMOLE a créé un axe, voire un domaine de recherche original, avec un positionnement spécifique dont l’un des aspects remarquables est un haut niveau d'interdisciplinarité entre sciences humaines et disciplines plus techniques comme l’informatique. Selon Banos (2017) cela conduit à la production d’une connaissance plus large et plus profonde (à l’image de la spirale vertueuse de Banos (2013) entre disciplinarité et interdisciplinarité). Mais aussi, avec la philosophie de plateforme unique (évoquée ci-dessus, par l’interaction forte entre les trois axes d’embarquement des modèles, d'accès à des méthodes d’exploration innovantes, et d'accès transparent aux environnements de calcul intensifs), les perspectives ouvertes sont nombreuses, tant sur le plan technique que sur celui théorique, méthodologique ou thématique. Nous en donnons ci-dessous quelques illustrations, rendant compte d’un état présent des futurs possibles pour OpenMOLE.
The elaboration of the OpenMOLE platform has created a research axis, even a research domain, with a specific positioning which one of the remarkable aspects is a high level of interdisciplinarity between social sciences and more technical disciplines such as computer science. According to \cite{banos2017knowledge} this leads to the production of a broader and deeper knowledge (in a way similar to the virtuous spiral between disciplinarity and interdisciplinarity described by \cite{banos2013pour}). But also through the philosophy of unique platform (described above, through the strong interaction between the three axis of model embedding, access to innovative model exploration methods, and transparent access to intensive computation environments), the perspectives opened are numerous, as much on the technical side than on the theoretical, methodological or thematic side. We give below a few examples, accounting of a current state of possible futures for OpenMOLE.


%5.1 Méthodes
\subsection{Methods}

%L’extension des méthodes mises à disposition est un axe privilégié de la recherche liée au développement d’OpenMOLE. Par exemple, la résolution exhaustive de problèmes inverses (Aster et al. 2018) n’est actuellement pas incluse. La résolution d’un problème inverse consiste a déterminer l’ensemble des antécédents d’un objectif donné dans l’espace de sortie du modèle. Les algorithmes de calibrage résolvent des problèmes similaires mais ne garantissent pas l'exhaustivité des solutions produites, ce qui peut considérablement poser problème en cas d'équifinalité (Rey-Coyrehourcq, 2015), i.e. de configurations de paramètres ou de conditions initiales conduisant par des trajectoires différentes à un résultat identique. Une heuristique de problème inverse s’inspirant des mécanismes de PSE est actuellement en cours d'élaboration pour une intégration dans OpenMOLE.
The extension of available methods is a privileged axis of research linked to the development of OpenMOLE. For example, the exhaustive resolution of inverse problems \citep{aster2018parameter} is currently not included. Solving an inverse problem consists in determining all the antecedents of a given objective in the output space of the model. Calibration algorithms solve similar problems but do not ensure the exhaustivity of the solutions produced, what can become a considerable issue in the case of equifinality \citep{rey2015plateforme}, i.e. of parameter configurations or initial conditions leading through different trajectories to an identical result. An heuristic for inverse problems inspired by the PSE mechanisms is currently being elaborated for an integration into OpenMOLE.


%L’utilisation de méthodes d'inférence Bayésiennes est également une piste développée. En effet, dans le cas de modèles fortement stochastiques, et où les distributions jointes ont des formes non standard, une estimation de la distribution de probabilité des paramètres peut être fournie par ce type de méthodes. Dans le cas des modèles de simulation, la méthode d’Approximate Bayesian Computation (Csilléry et al. 2010)  permet, pour un jeu de données observées, de fournir la distribution de probabilité des paramètres ayant le plus probablement conduit à celles-ci. Il s’agit ainsi d’un calibrage étendu, avec une connaissance produite probabiliste permettant de prendre en compte l’incertitude. Une spécification de cette méthode proposée par Lenormand et al. (2013), destinée à réduire le nombre de simulations dans le cas de modèles au temps de calcul significatif, est également en cours d’adaptation au calcul parallèle et d'intégration dans la plateforme.
The use of Bayesian inference methods is also a direction developed. Indeed, in the case of strongly stochastic models, and in which the joint distributions have a non standard form, an estimation of the probability distribution of parameters can be provided by this type of method. In the case of simulation models, the method of Approximate Bayesian Computation \citep{csillery2010approximate} allows for a given observed dataset to get the probability distribution of parameters having the most likely produced it. This is therefore an extended calibration with a probabilistic knowledge produced allowing to take into account uncertainty. A specification of this method proposed by \cite{lenormand2013adaptive} with the purpose to reduce the number of simulations in the case of models with a significant computation time, is also being adapted to parallel computation and integrated into the platform.


%Signalons finalement diverse directions méthodologiques également en cours d’investigation: (i) la question de la haute dimensionnalité pose rapidement problème dans l’utilisation de l’algorithme PSE, puisque le nombre de configurations de sortie est potentiellement soumis à la malédiction de la dimension (curse of dimensionality) c’est-à-dire que le temps ou la taille d'exécution sont exponentiels en le nombre de dimension (une exploration par grille est l’exemple le plus simple pour se donner une idée de ce phénomène) - de nouvelles méthodes combinant réduction de dimension et recherche de diversité permettraient de résoudre ce problème et prendre en compte une richesse de sorties bien plus grande; (ii) la question de la sensibilité aux conditions spatiales initiales déjà mentionnée (Raimbault et al., 2018), est particulièrement pertinente pour les modèles géographiques, et une librairie Scala incluant des générateurs synthétiques de configurations de peuplement à différentes échelles est actuellement en cours d'élaboration, incluant par exemple les générateurs de quartiers étudiés par (Raimbault and Perret, 2019); (iii) l'implémentation de critères d’information pour la performance des modèles, déjà mentionnés dans le chapitre 4 et qui sont une pierre angulaire des démarches de multi-modélisation, est aussi à l'étude, comme le critère POMIC proposé par Piou et al. (2009).
We can finally mention diverse methodological directions which are also being investigated: (i) the question of high dimensionality is rapidly an issue in the use of the PSE algorithm, since the number of output configurations is potentially victim of the dimensionality curse, i.e. that the time or the size of execution are an exponential function of the number of dimensions (a grid exploration is the simplest example to get a grasp on this phenomenon) - new methods combining dimensionality reduction and diversity search would allow to solve this problem and take into account a much higher richness of outputs; (ii) the question of the sensitivity to initial spatial conditions which was already mentioned \citep{raimbault2018space} is particularly relevant for geographical models, and a scala library including synthetic generators for population configurations at different scales is currently being implemented, including for example the generators for districts studied by \cite{raimbault2019generating}; (iii) the implementation of information criteria for the performance of models, already described in chapter 4, is also being studied, such as the POMIC criteria proposed by \cite{piou2009proposing}.



%5.2 Outils
\subsection{Tools}


%Au long de son développement, OpenMOLE a toujours été à la pointe en termes d’outils utilisés et développés. Le choix du langage Scala pour remplacer Java dès les premières versions, est un choix technologique innovant et particulièrement pertinent par les possibilités de programmation fonctionnelle mais aussi de programmation objet qu’il apporte, tout en gardant l’infrastructure sous-jacente de Java permettant une grande portabilité sans complications selon le système d’exploitation ou le hardware, ce qui est crucial pour la distribution des calculs sur des noeuds de grille hétérogènes. Par exemple, des propriétés comme le mixage de trait rendent Scala particulièrement pertinent pour la multi-modélisation (Odersky and Zenger, 2005). Les possibilités offertes par la programmation objet sont conservées dans Scala, et peuvent être combinées à l’abstraction de la programmation fonctionnelle, en faisant un langage plus puissant en ce sens de flexibilité que d’autres langages fonctionnels comme Haskell (Oliveira and Gibbons, 2010). Par ailleurs, des propriétés comme les conversions implicites ou les case class rendent Scala ergonomique pour l'élaboration de DSL (Sloane, 2008), qui comme nous l’avons déjà mentionné est un aspect essentiel d’OpenMOLE.
All along its development, OpenMOLE has always been innovative in terms of tools used and developed. The choice of the Scala language to replace Java already in the first versions is an innovative technological choice which is particularly relevant through the functional programming but also object programming possibilities while still keeping the underlying Java infrastructure allowing a high portability without complications depending on the operating system or on the hardware, what is crucial for the distribution of computations of computations on heterogenous nodes of the computation grid. For example, properties such as trait mixing make scala particularly suited to multi-modeling \citep{odersky2005scalable}. The possibilities offered by object programming are conserved in Scala and can be combined to the abstraction of functional programming, making it a language more powerful in this sense of flexibility than other functional languages such as Haskell~\citep{oliveira2010scala}. Furthermore, properties such as implicit conversions or case classes make Scala highly ergonomic for the design of DSL \citep{sloane2008experiences}, which as we already described is an essential feature of OpenMOLE.


%Les questions d’embarquement de programmes, et par extension de modèles, restent un domaine de recherche actif notamment en lien avec la reproductibilité. Le programme docker, qui utilise des container, permet d’embarquer un environnement d'exécution à l’identique quel que soit le système d’exploitation et le hardware. Hung et al. (2016) propose de coupler docker à une interface graphique pour la reproductibilité scientifique. Des programmes similaires comme Singularity sont spécifiquement dédiés à la reproductibilité d'expériences HPC (Kurtzer et al., 2017). Le coeur de la stratégie d’embarquement d’OpenMOLE ne repose pas sur un tel programme, par exemple pour des questions de performance, mais certaines tâches reposant sur l'exécution de binaires ou de programme à l’environnement complexe sont embarquées dans OpenMOLE par une tâche utilisant docker (par exemple pour la tâche pour le langage R qui demande l’installation d’un environnement R complet). Une amélioration de l'intégration de docker dans OpenMOLE est un axe de recherche actif et crucial pour l’extension future de la généricité des programmes embarquables. OpenMOLE se place ainsi à la pointe de la recherche technique en termes de reproductibilité scientifique. De la même manière, la question de la scalabilité des expériences est au coeur de la philosophie de la plateforme, et des recherches sont menées par exemple pour automatiser le déploiement de multiples instances d’OpenMOLE sur un cluster et faciliter l’utilisation au sein de communautés de thématiciens.
The issue of program embedding, and by extension of model embedding, remain an active research field in particular in relation with reproducibility. The docker software which uses containers allows to wrap an execution environment in an identical manner whatever the operating system and the hardware. \cite{hung2016guidock} propose to couple docker with a graphical user interface for scientific reproducibility. Similar softwares such as Singularity are specifically dedicated to the reproducibility of HPC experiments \citep{kurtzer2017singularity}. The core of the embedding strategy taken by OpenMOLE does not rely on such a software, for example because of performance reasons, but some tasks relying on the execution of binaries or of programs with a complicated environment are embedded in OpenMOLE through a task using docker (for example for the R language task which requires the installation of a full R environment). An improvement of the integration of docker into OpenMOLE is an active research direction which is crucial for the future extension of the genericity of embeddable programs. OpenMOLE is therefore at the edge in technical research regarding scientific reproducibility. In a similar way, the question of scalability of experiments is at the core of the philosophy of the platform, and research are done for example to automatize the deployment of multiple OpenMOLE instances on a cluster and facilitate the use within communities of thematic researchers.




\section*{Conclusion}


%L’exploration des modèles de simulation s’est pérennisée en géographie par l'intermédiaire d’initiatives comme le développement de la plateforme OpenMOLE. Celle-ci s’est menée dans un cadre hautement interdisciplinaire et réciproque (relation gagnant-gagnant entre informaticiens et géographes), mais aussi au travers d’une intégration inédite des domaines de connaissance (Raimbault, 2017a), c’est-à-dire des connaissances empiriques, théoriques et de modélisation, mais aussi les outils et méthodes, qui sont dans chacun de ces domaines en interaction forte. L’aventure OpenMOLE, et sa branche liée à la géographie dans le cadre de l’ERC Geodivercity, témoigne d’une nouvelle façon de produire des connaissances géographiques, résolument evidence-based, rendant envisageable la production de preuves scientifiques en sciences sociales. Cette émancipation reste à être propagée et la démarche à être valorisée pour réaliser son potentiel de direction future de la Géographie Théorique et Quantitative, en complémentarité avec les nouvelles disciplines émergentes de City Science et Urban Analytics décrites par Batty (2019), mais la “preuve de concept” est largement validée et donne des arguments de poids aux sciences humaines pour résister à l'hégémonie colonisatrice de sciences dures comme la physique prétendant à un monopole sur les approches evidence-based des systèmes sociaux (Dupuy and Benguigui, 2015).
The exploration of simulation models has been progressively established in geography through the intermediary of initiatives such as the development of the OpenMOLE platform. It has been achieved in a highly interdisciplinary and reciprocal framework (win-win relations between computer scientists and geographers), but also through a novel integration of knowledge domains \citep{raimbault2017applied}, i.e. of empirical, theoretical and modeling knowledges, but also tools and methods which are within each of these domains in strong interaction. The OpenMOLE enterprise and its branch linked to geography in the context of the Geodivercity ERC project witnesses of a novel way to produce geographical knowledge, in a robust evidence-based manner, and suggesting the possibility of scientific proofs in social sciences. Such an emancipation remains to be propagated and the approach to be valorized to realize its potential of future direction of Quantitative and Theoretical Geography, in complementarity with new emerging disciplines of City Science and Urban Analytics described by \cite{batty2019urban}, but the proof-of-concept is largely validated and provides significant evidence to social sciences to resist the colonizer hegemony of hard sciences such as physics pretending to a monopoly on evidence-based approaches to social systems \citep{dupuy2015sciences}.



\bibliographystyle{apalike}
\bibliography{biblio}



\end{document}



